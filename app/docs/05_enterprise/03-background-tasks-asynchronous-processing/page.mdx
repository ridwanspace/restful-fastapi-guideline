# âš¡ Background Tasks and Asynchronous Processing

In web applications, especially APIs, it's crucial to respond to client requests quickly. Some operations, however, can take a significant amount of time to complete (e.g., sending an email, processing a large file, calling a slow third-party API). Performing these tasks synchronously within the request-response cycle can lead to long wait times for the client and potentially request timeouts. Background tasks allow you to offload these operations, respond immediately to the client, and process the work independently.

### When to Use Background Tasks
*   **Sending Notifications:** Emails, SMS, push notifications.
*   **Data Processing:** Generating reports, resizing images, data aggregation.
*   **Third-Party API Calls:** Interacting with external services that might be slow or unreliable.
*   **Cache Invalidation/Population:** Updating caches after data changes.
*   **Logging and Analytics:** Sending detailed logs or analytics events to a separate system.
*   Any operation that doesn't need to complete before sending a response to the client.

### FastAPI's `BackgroundTasks` Dependency

FastAPI provides a simple way to run tasks in the background using the `BackgroundTasks` object. These tasks run *within the same process* as your FastAPI application after the response has been sent.

**Key Features:**
*   Easy to use: Inject `BackgroundTasks` into your path operation function.
*   Runs after response: Ensures the client gets a quick acknowledgment.
*   Suitable for I/O-bound tasks that are not CPU-intensive and don't require a separate worker process.

**Limitations:**
*   **Single Process:** Tasks run in the same event loop and process as the main application. CPU-bound tasks can still block the event loop if not handled carefully (e.g., by running them in a thread pool executor via `asyncio.to_thread`).
*   **No Built-in Retries/Persistence:** If the server crashes while a background task is running, the task is lost. There's no automatic retry mechanism.
*   **Limited Monitoring:** No built-in dashboard or extensive monitoring for these tasks.
*   **Not for Very Long-Running or Critical Tasks:** For tasks that are critical, very long-running, or require guaranteed execution, a dedicated task queue is more appropriate.

**Example: Sending a Confirmation Email**
```python
from fastapi import FastAPI, BackgroundTasks, Depends, status
from pydantic import BaseModel
import time
import logging
import uvicorn 
import asyncio 

app = FastAPI()
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

async def send_email_async(email_to: str, subject: str, body: str):
    logger.info(f"Preparing to send email to: {email_to} with subject: '{subject}'")
    await asyncio.sleep(5) 
    logger.info(f"Email successfully sent to: {email_to}, Subject: '{subject}', Body: '{body[:50]}...'")

def send_email_sync(email_to: str, subject: str, body: str):
    logger.info(f"SYNC: Preparing to send email to: {email_to} with subject: '{subject}'")
    time.sleep(5) 
    logger.info(f"SYNC: Email successfully sent to: {email_to}, Subject: '{subject}', Body: '{body[:50]}...'")


class UserCreate(BaseModel):
    username: str
    email: str
    password: str

@app.post("/users/register", status_code=status.HTTP_201_CREATED)
async def register_user(user_data: UserCreate, background_tasks: BackgroundTasks):
    logger.info(f"Registering user: {user_data.username} with email: {user_data.email}")
    user_id = 123 

    email_subject = "Welcome to Our Awesome Service!"
    email_body = f"Hi {user_data.username},\n\nWelcome! We're glad to have you."
    
    background_tasks.add_task(send_email_async, user_data.email, email_subject, email_body)
    
    logger.info(f"User {user_data.username} registration request processed. Email task added to background.")
    return {"message": "User registered successfully. Confirmation email will be sent.", "user_id": user_id}

# if __name__ == "__main__":
#     uvicorn.run(app, host="0.0.0.0", port=8000)
```
In this example, the `/users/register` endpoint will immediately return a response to the client after adding the `send_email_async` task. The email sending will happen in the background.

### Integrating with Task Queues (e.g., Celery, RQ, Dramatiq, ARQ)

For more robust, scalable, and reliable background task processing, dedicated task queues are recommended. They offer:
*   **Separate Worker Processes:** Tasks run in different processes (or even different machines), preventing them from impacting API responsiveness.
*   **Persistence:** Tasks can be stored in a message broker (e.g., Redis, RabbitMQ) and will be processed even if the API server restarts.
*   **Retries & Error Handling:** Built-in mechanisms for retrying failed tasks with configurable strategies (e.g., exponential backoff).
*   **Scheduling:** Ability to schedule tasks to run at specific times or intervals.
*   **Monitoring & Management:** Tools to inspect queues, workers, and task statuses (e.g., Flower for Celery).
*   **Distributed Execution:** Scale workers independently of your API servers.

**Conceptual Example: FastAPI with Celery**

This requires Celery and a message broker (like Redis or RabbitMQ) to be set up.

**1. Define Celery Tasks (`tasks.py`):**
```python
# tasks.py
from celery import Celery
import time
import logging
import os 

logger = logging.getLogger(__name__) 
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(task_id)s - %(message)s')


CELERY_BROKER_URL = os.environ.get("CELERY_BROKER_URL", "redis://localhost:6379/0")
CELERY_RESULT_BACKEND = os.environ.get("CELERY_RESULT_BACKEND", "redis://localhost:6379/0")

celery_app = Celery(
    "my_fastapi_tasks", 
    broker=CELERY_BROKER_URL,
    backend=CELERY_RESULT_BACKEND
)

celery_app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="UTC",
    enable_utc=True,
    task_track_started=True, 
)


@celery_app.task(bind=True, name="send_confirmation_email_task", max_retries=3, default_retry_delay=60) 
def send_confirmation_email_task(self, user_email: str, username: str):
    try:
        logger.info(f"Task {self.request.id}: Sending confirmation email to {user_email} for user {username}")
        time.sleep(10) 
        logger.info(f"Task {self.request.id}: Confirmation email sent to {user_email}")
        return {"status": "success", "email": user_email, "task_id": self.request.id}
    except Exception as e:
        logger.error(f"Task {self.request.id}: Failed to send email to {user_email}: {e}", exc_info=True)
        try:
            raise self.retry(exc=e, countdown=self.default_retry_delay * (2**self.request.retries))
        except self.MaxRetriesExceededError:
            logger.critical(f"Task {self.request.id}: Max retries exceeded for sending email to {user_email}.")
            return {"status": "failed_max_retries", "email": user_email, "task_id": self.request.id, "error": str(e)}


@celery_app.task(bind=True, name="process_large_data_task")
def process_large_data_task(self, data_id: str, user_id: int):
    logger.info(f"Task {self.request.id}: Starting processing for data_id: {data_id}, user_id: {user_id}")
    total_steps = 10
    for i in range(total_steps):
        time.sleep(2) 
        self.update_state(state='PROGRESS', meta={'current': i + 1, 'total': total_steps, 'status': f'Processing step {i+1}'})
        logger.info(f"Task {self.request.id}: Step {i+1}/{total_steps} for data_id: {data_id}")
    
    result_url = f"/results/data/{data_id}" 
    logger.info(f"Task {self.request.id}: Finished processing for data_id: {data_id}. Result at {result_url}")
    return {"status": "completed", "data_id": data_id, "result_url": result_url, "task_id": self.request.id}

```

**2. Trigger Celery Task from FastAPI Endpoint (`main.py`):**
```python
# main.py (FastAPI app)
from fastapi import FastAPI, BackgroundTasks, HTTPException, status as http_status
from pydantic import BaseModel
import tasks 
import logging
import uvicorn
from celery.result import AsyncResult 

app = FastAPI()
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

class UserRegistration(BaseModel):
    username: str
    email: str

class DataProcessingRequest(BaseModel):
    data_id: str
    user_id: int

@app.post("/users/register-celery")
async def register_user_with_celery(user_data: UserRegistration):
    logger.info(f"Received registration for {user_data.username}, queuing email task.")
    task_result = tasks.send_confirmation_email_task.delay(user_data.email, user_data.username)
    logger.info(f"Email task for {user_data.email} enqueued with Celery Task ID: {task_result.id}")
    return {
        "message": "User registration submitted, confirmation email will be sent.",
        "email_task_id": task_result.id
    }

@app.post("/process-data-celery")
async def process_data_with_celery(data_request: DataProcessingRequest):
    logger.info(f"Received data processing request for data_id: {data_request.data_id}")
    task_result = tasks.process_large_data_task.delay(data_request.data_id, data_request.user_id)
    logger.info(f"Data processing task for {data_request.data_id} enqueued with Celery Task ID: {task_result.id}")
    return {
        "message": "Data processing task initiated.",
        "task_id": task_result.id,
        "status_url": f"/tasks/status/{task_result.id}" 
    }

@app.get("/tasks/status/{task_id}")
async def get_task_status(task_id: str):
    task = AsyncResult(task_id, app=tasks.celery_app) 
    
    response = {
        "task_id": task_id,
        "status": task.status,
        "result": task.result if task.ready() else None,
    }
    if task.status == 'PROGRESS':
        response['progress'] = task.info 
    elif task.status == 'FAILURE':
        response['error_info'] = str(task.result) 
    
    return response

# To run this:
# 1. Start Redis: `redis-server`
# 2. Start Celery worker: `celery -A tasks.celery_app worker -l info --concurrency=4`
# 3. Start FastAPI app: `uvicorn main:app --reload`
```

### Handling Long-Running Operations & Status Updates
When a task is offloaded, the client might need to know its status or get the result later.
*   **Return Task ID:** As shown in the Celery example, return a unique task ID to the client.
*   **Status Endpoint:** Provide an endpoint (e.g., `/tasks/status/{task_id}`) where the client can poll for the task's status and result.
*   **WebSockets/Server-Sent Events (SSE):** For real-time updates, the server can push status updates to the client over a WebSocket connection or using SSE once the task state changes. This avoids client-side polling.
*   **Callbacks/Webhooks:** The background task, upon completion (or failure), can make an HTTP request to a callback URL provided by the client or another service.

### Monitoring Background Tasks
*   **Logging:** Implement comprehensive logging within your task functions. Include task IDs, parameters, and timing information.
*   **Task Queue Monitoring Tools:**
    *   **Flower:** A web-based tool for monitoring and administrating Celery clusters.
    *   **RQ Dashboard:** For Redis Queue.
    *   Many task queues have similar UIs or CLI tools.
*   **Application Performance Monitoring (APM):** Integrate with APM tools (e.g., Datadog, New Relic, Sentry) which often have specific integrations for popular task queues. This allows you to trace tasks, view error rates, and measure performance.
*   **Metrics:** Collect metrics like queue length, number of active workers, task execution time, and error rates. Expose these via a metrics endpoint (e.g., for Prometheus).
*   **Alerting:** Set up alerts for critical failures, long queue times, or high error rates.

---