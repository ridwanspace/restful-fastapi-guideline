# ğŸ—ï¸ From Code to Cloud Empire: Mastering Container Orchestration at Scale

*Transform your FastAPI application from a single village building into a resilient global empire*

## ğŸ¯ The Great Digital Transformation Journey

### ğŸ° Real-World Analogy: Building a Global Digital Empire

Think of yourself as an ambitious ruler who starts with a small village (your FastAPI app) and dreams of building a vast, interconnected empire that spans continents. This journey mirrors how modern applications evolve from simple scripts to globally distributed systems.

- **ğŸ˜ï¸ Your Local Development**: A cozy village where you know every citizen personally
- **ğŸ³ Containers (Docker)**: Standardized city modules that work anywhere in your empire
- **â˜¸ï¸ Orchestration (Kubernetes)**: Your imperial administration that manages millions of citizens across thousands of cities
- **ğŸŒ Production Deployment**: A thriving global empire serving citizens worldwide

Just as Rome wasn't built in a day, container mastery requires understanding each building block and how they work together to create something greater than the sum of their parts.

### ğŸ“Š The Imperial Architecture Evolution

```mermaid
graph TD
    A[ğŸ  Village Hut<br/>Local Development] --> B[ğŸ˜ï¸ Planned Neighborhood<br/>Docker Basics]
    B --> C[ğŸ™ï¸ Metropolitan City<br/>Docker Compose]
    C --> D[ğŸ›ï¸ Imperial Province<br/>Kubernetes Cluster]
    D --> E[ğŸŒ Global Empire<br/>Multi-Region Production]
    
    A1[ğŸ‘¨â€ğŸ’» Single Developer<br/>Your laptop] --> A
    B1[ğŸ“¦ Portable Modules<br/>Container Images] --> B
    C1[ğŸ”— Connected Services<br/>Multi-container Apps] --> C
    D1[âš–ï¸ Auto-scaling<br/>Self-healing Systems] --> D
    E1[ğŸŒ Global Scale<br/>Enterprise Operations] --> E
    
    style A fill:#fff3e0,color:#3b3b3b
    style B fill:#e8f5e8,color:#3b3b3b
    style C fill:#e3f2fd,color:#3b3b3b
    style D fill:#f3e5f5,color:#3b3b3b
    style E fill:#ffebee,color:#3b3b3b
```

### ğŸ­ Learning Paths: Choose Your Imperial Journey

This guide supports multiple learning approaches based on your current experience:

**ğŸŒ± New to Containers** (Start with Village Planning)
- Begin with **Container Fundamentals**
- Master **Docker Basics** before moving to orchestration
- Focus on understanding the **"why"** behind each concept

**ğŸ—ï¸ Docker Experience** (Jump to City Management)
- Skip to **Multi-Container Applications**
- Focus on **Production Best Practices**
- Implement **Advanced Orchestration Patterns**

**ğŸ›ï¸ Production Ready** (Master Imperial Operations)
- Jump to **Enterprise Deployment Strategies**
- Focus on **Security and Compliance**
- Implement **Global Scale Patterns**

ğŸ’¡ **Pro Tip**: Even if you're experienced, reviewing the analogies and fundamentals often reveals insights you missed before.

## ğŸ—ï¸ Chapter 1: The Foundation - Container Fundamentals

### ğŸ¯ The Village Planning Problem

Imagine trying to build a village where every house uses different electrical systems, plumbing standards, and building materials. When you want to relocate a house to a new plot of land, you have to rebuild everything from scratch. This is exactly the problem containers solve.

**The Traditional Chaos:**
- "It works on my machine" but fails everywhere else
- Each environment requires different setup procedures
- Dependency conflicts between applications
- Manual configuration that's error-prone and time-consuming

**The Container Solution:**
- Every building (application) is self-contained with all its utilities
- Buildings work identically whether placed in a testing ground or production city
- No conflicts between neighboring buildings
- Simple, automated placement and relocation

### ğŸ“š What Are Containers? The Imperial Standard

A container is like a **standardized, prefabricated building module** that includes:

ğŸ  **The Structure**: Your application code
ğŸ”Œ **All Utilities**: Runtime dependencies and libraries  
ğŸ› ï¸ **Internal Systems**: Configuration and environment setup
ğŸ“‹ **Assembly Instructions**: How to start and run the application

### ğŸ›ï¸ The Container Advantage Empire

| ğŸ¯ **Benefit** | ğŸ° **Imperial Analogy** | ğŸ’¼ **Business Impact** | ğŸ”§ **Technical Reality** |
|----------------|--------------------------|-------------------------|---------------------------|
| **ğŸ”„ Consistency** | Same building design works in any province | Eliminates "works on my machine" problems | Identical environments from dev to production |
| **ğŸ“¦ Portability** | Move buildings between cities instantly | Deploy anywhere: cloud, on-premise, hybrid | Abstract away infrastructure differences |
| **âš¡ Efficiency** | Lighter than full city construction | Faster startups, less resource usage | Share host OS, not full virtualization |
| **ğŸ›¡ï¸ Isolation** | Each building has its own utilities | Security and stability improvements | Process and network isolation |
| **ğŸ“ˆ Scalability** | Build identical districts instantly | Handle traffic spikes automatically | Horizontal scaling made simple |

### ğŸ”§ Your First Imperial Building: Dockerizing FastAPI

Let's construct your first standardized building using Docker. This will be the template for all future constructions in your empire.

#### ğŸ—ï¸ The Master Blueprint (Production-Grade Dockerfile)

```dockerfile
# Dockerfile - The Complete Imperial Building Specification

# ğŸ¯ STAGE 1: The Royal Construction Site
# Analogy: A temporary workshop where we gather all tools and materials
# We'll discard this site after extracting what we need, keeping the final building clean
FROM python:3.11-slim-bullseye AS builder

# Set construction site environment variables
# Like establishing work rules for our construction crew
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PYTHONFAULTHANDLER=1 \
    PYTHONHASHSEED=random \
    PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1

# Create the construction workspace
WORKDIR /build

# Install essential construction tools
# We need these to compile any native dependencies
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy the materials list and create a secure toolkit (virtual environment)
# This isolates our building's tools from the base system
COPY requirements.txt requirements-dev.txt ./
RUN python -m venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Install our materials with optimizations
# Using --no-deps with pip-tools for reproducible builds
RUN pip install --upgrade pip setuptools wheel && \
    pip install -r requirements.txt

# ğŸ¯ STAGE 2: The Imperial Palace (Production Runtime)
# Analogy: The final, secure, and optimized building ready for citizens
FROM python:3.11-slim-bullseye AS runtime

# Set runtime environment variables
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PYTHONFAULTHANDLER=1 \
    APP_HOME=/app \
    PYTHONPATH=/app

# Install only essential runtime dependencies
# Like ensuring our building has basic utilities but no construction debris
RUN apt-get update && apt-get install -y \
    --no-install-recommends \
    curl \
    && rm -rf /var/lib/apt/lists/* \
    && apt-get clean

# Create a dedicated building operator (non-root user for security)
# Analogy: We don't give the master key to daily building operators
RUN groupadd --gid 1000 appuser && \
    useradd --uid 1000 --gid appuser --shell /bin/bash --create-home appuser

# Set up the building workspace
WORKDIR $APP_HOME

# Copy the pre-assembled toolkit from our construction site
# This transfers our Python environment without the build tools
COPY --from=builder /opt/venv /opt/venv
ENV PATH="/opt/venv/bin:$PATH"

# Copy the application blueprints (source code)
# Do this after installing dependencies to leverage Docker's layer caching
COPY --chown=appuser:appuser . $APP_HOME/

# Ensure our building operator owns everything
RUN chown -R appuser:appuser $APP_HOME

# Switch to our secure building operator
USER appuser

# Declare the building's public entrance (expose port)
# Like putting up a sign saying "Main Entrance: Port 8000"
EXPOSE 8000

# Add health check - the building's self-diagnostic system
# Kubernetes and other orchestrators use this to verify building health
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health/live || exit 1

# The grand opening command - how to activate the building
# Using Gunicorn for production-grade WSGI serving
CMD ["gunicorn", "main:app", "-w", "4", "-k", "uvicorn.workers.UvicornWorker", "--bind", "0.0.0.0:8000", "--access-logfile", "-", "--error-logfile", "-"]
```

#### ğŸ“‹ Essential Supporting Documents

**requirements.txt - The Materials Manifest**
```txt
# Core empire infrastructure
fastapi==0.104.1
uvicorn[standard]==0.24.0
gunicorn==21.2.0

# Database empire connections
sqlalchemy==2.0.23
alembic==1.12.1
asyncpg==0.29.0        # For PostgreSQL
redis==5.0.1           # For caching

# Security and validation
pydantic==2.5.0
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4
python-multipart==0.0.6

# Monitoring and observability
prometheus-client==0.19.0
opentelemetry-api==1.21.0
opentelemetry-sdk==1.21.0

# Add your specific empire requirements here
```

**requirements-dev.txt - Development Tools**
```txt
# Testing arsenal
pytest==7.4.3
pytest-asyncio==0.21.1
pytest-mock==3.12.0
httpx==0.25.2          # For testing async endpoints

# Code quality guardians
black==23.11.0         # Code formatting
isort==5.12.0          # Import sorting
flake8==6.1.0          # Linting
mypy==1.7.1            # Type checking
bandit==1.7.5          # Security scanning

# Development utilities
pre-commit==3.6.0      # Git hooks
coverage==7.3.2        # Test coverage
```

**.dockerignore - The Imperial Exclusion List**
```
# Exclude temporary construction debris
__pycache__/
*.pyc
*.pyo
*.pyd
.Python
*.so

# Development environments and secrets
.env
.env.*
env/
venv/
ENV/
env.bak/
venv.bak/
.venv/

# Version control and documentation
.git/
.gitignore
.gitattributes
README.md
docs/
.github/

# IDE and editor files
.vscode/
.idea/
*.swp
*.swo
*~

# Testing and coverage
.pytest_cache/
.coverage
htmlcov/
.tox/
.cache
nosetests.xml
coverage.xml
*.cover
.hypothesis/

# Build artifacts
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Deployment files (we'll handle these separately)
docker-compose*.yml
Dockerfile
k8s/
```

### âœ… Imperial Construction Commands

**1. Build Your Imperial Standard Building**
```bash
# Build with optimized layer caching and buildx for multi-platform support
docker buildx build \
  --platform linux/amd64,linux/arm64 \
  --tag my-fastapi-empire:latest \
  --tag my-fastapi-empire:$(git rev-parse --short HEAD) \
  --cache-from type=local,src=/tmp/.buildx-cache \
  --cache-to type=local,dest=/tmp/.buildx-cache \
  --push \
  .

# For local development (single platform)
docker build -t my-fastapi-empire:dev .
```

**2. Deploy Your First Building**
```bash
# Run with proper resource limits and health monitoring
docker run -d \
  --name fastapi-palace \
  --publish 8000:8000 \
  --memory 512m \
  --cpus 1.0 \
  --restart unless-stopped \
  --health-cmd "curl -f http://localhost:8000/health/live || exit 1" \
  --health-interval 30s \
  --health-retries 3 \
  --health-start-period 10s \
  --env-file .env.production \
  my-fastapi-empire:latest
```

**3. Imperial Monitoring Commands**
```bash
# Check building health status
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"

# View building logs (like reading the royal chronicles)
docker logs -f --tail 100 fastapi-palace

# Enter the building for inspection (debugging)
docker exec -it fastapi-palace /bin/bash

# Monitor resource usage (like checking the royal treasury)
docker stats fastapi-palace
```

### ğŸ¯ Building Your First Imperial Health System

Add these essential endpoints to your FastAPI application for proper imperial monitoring:

```python
# health.py - The Imperial Health Monitoring System
from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.ext.asyncio import AsyncSession
import redis.asyncio as redis
import asyncio
import logging
from datetime import datetime
from typing import Dict, Any
import os

# Create the health monitoring router
health_router = APIRouter(prefix="/health", tags=["Imperial Health System"])

# Configure the imperial health logger
logger = logging.getLogger("imperial.health")

class HealthChecker:
    """
    Imperial Health Monitoring System
    
    Like having royal physicians who continuously monitor the health
    of all critical systems in your empire.
    """
    
    def __init__(self):
        self.checks = {}
        self.startup_time = datetime.utcnow()
    
    async def check_database_connection(self) -> Dict[str, Any]:
        """
        Check if our royal archives (database) are accessible.
        
        Like verifying that the royal scribes can access the record vaults.
        """
        try:
            # This would use your actual database session
            # from database import get_async_session
            # async with get_async_session() as session:
            #     await session.execute("SELECT 1")
            
            # For demo purposes, simulating a database check
            await asyncio.sleep(0.1)  # Simulate DB query time
            
            return {
                "status": "healthy",
                "response_time_ms": 100,
                "connection_pool": "optimal"
            }
        except Exception as e:
            logger.error(f"Database health check failed: {e}")
            return {
                "status": "unhealthy",
                "error": str(e),
                "response_time_ms": None
            }
    
    async def check_cache_connection(self) -> Dict[str, Any]:
        """
        Check if our royal messenger system (Redis) is operational.
        
        Like verifying that our fastest messengers can still deliver
        urgent communications across the empire.
        """
        try:
            redis_client = redis.from_url(
                os.getenv("REDIS_URL", "redis://localhost:6379"),
                encoding="utf-8",
                decode_responses=True
            )
            
            # Test Redis with a simple ping
            start_time = datetime.utcnow()
            await redis_client.ping()
            response_time = (datetime.utcnow() - start_time).total_seconds() * 1000
            
            await redis_client.close()
            
            return {
                "status": "healthy",
                "response_time_ms": round(response_time, 2),
                "connection": "active"
            }
        except Exception as e:
            logger.error(f"Cache health check failed: {e}")
            return {
                "status": "unhealthy", 
                "error": str(e),
                "response_time_ms": None
            }
    
    async def check_external_services(self) -> Dict[str, Any]:
        """
        Check critical external alliances (third-party APIs).
        
        Like sending envoys to verify that trade routes with
        allied kingdoms are still operational.
        """
        try:
            # This would check your external dependencies
            # For example, payment processors, email services, etc.
            
            # Simulated external service check
            await asyncio.sleep(0.2)
            
            return {
                "status": "healthy",
                "services": {
                    "payment_gateway": "operational",
                    "email_service": "operational",
                    "notification_service": "operational"
                }
            }
        except Exception as e:
            logger.error(f"External services check failed: {e}")
            return {
                "status": "degraded",
                "error": str(e),
                "services": {}
            }

# Initialize the health checker
health_checker = HealthChecker()

@health_router.get("/live")
async def liveness_check():
    """
    Liveness Probe: The "Is the empire still standing?" check.
    
    This is the most basic check - it only verifies that the application
    process is running and can respond to requests. If this fails,
    Kubernetes will restart the container.
    
    Analogy: Like checking if the lights are on in the royal palace.
    """
    return {
        "status": "alive",
        "timestamp": datetime.utcnow().isoformat(),
        "uptime_seconds": (datetime.utcnow() - health_checker.startup_time).total_seconds(),
        "version": os.getenv("APP_VERSION", "development")
    }

@health_router.get("/ready")
async def readiness_check():
    """
    Readiness Probe: The "Can the empire serve its citizens?" check.
    
    This comprehensive check verifies that all critical systems are
    operational and the application can handle traffic. If this fails,
    Kubernetes stops sending traffic until it recovers.
    
    Analogy: Like verifying the royal court is fully staffed and
    ready to handle citizens' requests.
    """
    try:
        # Run all health checks concurrently for efficiency
        database_result, cache_result, external_result = await asyncio.gather(
            health_checker.check_database_connection(),
            health_checker.check_cache_connection(), 
            health_checker.check_external_services(),
            return_exceptions=True
        )
        
        # Determine overall health status
        all_checks = {
            "database": database_result,
            "cache": cache_result,
            "external_services": external_result
        }
        
        # Check if any critical systems are unhealthy
        critical_failures = []
        if isinstance(database_result, dict) and database_result.get("status") == "unhealthy":
            critical_failures.append("database")
        if isinstance(cache_result, dict) and cache_result.get("status") == "unhealthy":
            critical_failures.append("cache")
        
        # Determine overall status
        if critical_failures:
            overall_status = "not_ready"
            status_code = status.HTTP_503_SERVICE_UNAVAILABLE
        elif any(isinstance(check, dict) and check.get("status") == "degraded" for check in all_checks.values()):
            overall_status = "degraded"
            status_code = status.HTTP_200_OK  # Still accept traffic but warn
        else:
            overall_status = "ready"
            status_code = status.HTTP_200_OK
        
        response_data = {
            "status": overall_status,
            "timestamp": datetime.utcnow().isoformat(),
            "checks": all_checks,
            "critical_failures": critical_failures
        }
        
        if overall_status == "not_ready":
            raise HTTPException(status_code=status_code, detail=response_data)
        
        return response_data
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Readiness check failed unexpectedly: {e}")
        error_response = {
            "status": "not_ready",
            "timestamp": datetime.utcnow().isoformat(),
            "error": "Health check system failure",
            "detail": str(e)
        }
        raise HTTPException(
            status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
            detail=error_response
        )

@health_router.get("/detailed")
async def detailed_health_check():
    """
    Detailed Health Report: The complete imperial status report.
    
    This endpoint provides comprehensive health information for
    monitoring, alerting, and debugging purposes.
    
    Analogy: Like receiving a detailed report from all provincial
    governors about the state of their regions.
    """
    try:
        # Gather comprehensive system information
        basic_info = {
            "service": "FastAPI Imperial Application",
            "version": os.getenv("APP_VERSION", "development"),
            "environment": os.getenv("ENVIRONMENT", "development"),
            "startup_time": health_checker.startup_time.isoformat(),
            "uptime_seconds": (datetime.utcnow() - health_checker.startup_time).total_seconds(),
            "python_version": os.sys.version,
            "container_id": os.getenv("HOSTNAME", "unknown")
        }
        
        # Run all diagnostic checks
        database_result, cache_result, external_result = await asyncio.gather(
            health_checker.check_database_connection(),
            health_checker.check_cache_connection(),
            health_checker.check_external_services(),
            return_exceptions=True
        )
        
        return {
            "overall_status": "operational",
            "timestamp": datetime.utcnow().isoformat(),
            "basic_info": basic_info,
            "system_checks": {
                "database": database_result,
                "cache": cache_result,
                "external_services": external_result
            },
            "performance_metrics": {
                "memory_usage": "monitoring_not_implemented",
                "cpu_usage": "monitoring_not_implemented",
                "active_connections": "monitoring_not_implemented"
            }
        }
        
    except Exception as e:
        logger.error(f"Detailed health check failed: {e}")
        return {
            "overall_status": "error",
            "timestamp": datetime.utcnow().isoformat(),
            "error": str(e)
        }

# Remember to include this router in your main FastAPI app:
# app.include_router(health_router)
```

### ğŸ¯ Your First Imperial Achievement

ğŸ‰ **Congratulations!** You've just constructed your first standardized imperial building. Your FastAPI application is now:

- âœ… **Portable**: Works identically across all environments
- âœ… **Secure**: Runs with least-privilege user permissions
- âœ… **Optimized**: Multi-stage build reduces image size by ~60%
- âœ… **Observable**: Built-in health checks for monitoring
- âœ… **Production-Ready**: Proper logging, error handling, and resource limits

**ğŸš€ Next Steps**: Now that you have a standardized building, let's learn how to manage entire neighborhoods with Docker Compose.

---

## ğŸ™ï¸ Chapter 2: Neighborhood Planning - Multi-Container Orchestration

### ğŸ¯ The Metropolitan Challenge

A single building is impressive, but real empires are built from interconnected cities. Your FastAPI application needs supporting infrastructure: databases for record-keeping, caches for speed, message queues for communication, and monitoring systems for oversight.

**Docker Compose** is your metropolitan planning toolâ€”it lets you design, build, and manage entire neighborhoods of containers that work together seamlessly.

### ğŸ“Š The Imperial Neighborhood Architecture

```mermaid
graph TD
    subgraph "ğŸ° Imperial Neighborhood Network"
        subgraph "ğŸŒ External Access Layer"
            LB[ğŸšª Gateway<br/>Nginx Load Balancer<br/>Port 80/443]
        end
        
        subgraph "ğŸ›ï¸ Application Layer"
            API1[ğŸ¢ Palace Alpha<br/>FastAPI App<br/>Replica 1]
            API2[ğŸ¢ Palace Beta<br/>FastAPI App<br/>Replica 2]
            API3[ğŸ¢ Palace Gamma<br/>FastAPI App<br/>Replica 3]
        end
        
        subgraph "ğŸ’¾ Data Layer"
            DB[(ğŸ“š Royal Archives<br/>PostgreSQL<br/>Primary Database)]
            CACHE[(âš¡ Messenger Station<br/>Redis Cache<br/>High-Speed Storage)]
        end
        
        subgraph "ğŸ“Š Intelligence Layer"
            MON[ğŸ“ˆ Intelligence Hub<br/>Prometheus + Grafana<br/>Monitoring & Metrics]
            LOG[ğŸ“ Chronicle Keeper<br/>ELK Stack<br/>Centralized Logging]
        end
        
        subgraph "ğŸ”§ Support Services"
            QUEUE[(ğŸ“¬ Royal Post<br/>RabbitMQ<br/>Message Queue)]
            BACKUP[ğŸ’¿ Vault Keeper<br/>Backup Service<br/>Data Protection]
        end
    end
    
    LB --> API1
    LB --> API2
    LB --> API3
    
    API1 --> DB
    API2 --> DB
    API3 --> DB
    
    API1 --> CACHE
    API2 --> CACHE
    API3 --> CACHE
    
    API1 --> QUEUE
    API2 --> QUEUE
    API3 --> QUEUE
    
    MON -.-> API1
    MON -.-> API2
    MON -.-> API3
    MON -.-> DB
    MON -.-> CACHE
    
    LOG -.-> API1
    LOG -.-> API2
    LOG -.-> API3
    
    BACKUP -.-> DB
    
    style LB fill:#e3f2fd,color:#3b3b3b
    style API1 fill:#e8f5e8,color:#3b3b3b
    style API2 fill:#e8f5e8,color:#3b3b3b
    style API3 fill:#e8f5e8,color:#3b3b3b
    style DB fill:#bbdefb,color:#3b3b3b
    style CACHE fill:#ffecb3,color:#3b3b3b
    style MON fill:#f3e5f5,color:#3b3b3b
    style LOG fill:#fff3e0,color:#3b3b3b
```

### ğŸ”§ The Imperial Neighborhood Blueprint (Production Docker Compose)

This comprehensive setup creates a full production-like environment locally:

```yaml
# docker-compose.production.yml - The Complete Imperial Neighborhood
version: '3.8'

# ğŸŒ Imperial Networks - Secure Communication Channels
networks:
  imperial-frontend:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/24
  imperial-backend:
    driver: bridge
    internal: true  # Backend network is not accessible from outside
    ipam:
      config:
        - subnet: 172.21.0.0/24
  imperial-monitoring:
    driver: bridge
    ipam:
      config:
        - subnet: 172.22.0.0/24

# ğŸ—„ï¸ Imperial Vaults - Persistent Storage Systems
volumes:
  postgres_data:
    driver: local
  redis_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  elasticsearch_data:
    driver: local
  nginx_cache:
    driver: local

services:
  # ğŸšª The Imperial Gateway - Load Balancer & SSL Termination
  gateway:
    image: nginx:1.25-alpine
    container_name: imperial_gateway
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - nginx_cache:/var/cache/nginx
    networks:
      - imperial-frontend
    depends_on:
      - fastapi-app-1
      - fastapi-app-2
      - fastapi-app-3
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ğŸ¢ The Imperial Palaces - FastAPI Application Replicas
  fastapi-app-1: &fastapi-service
    build:
      context: .
      dockerfile: Dockerfile
      target: runtime
    container_name: imperial_palace_alpha
    restart: unless-stopped
    environment: &app-environment
      # Database Configuration
      - DATABASE_URL=postgresql+asyncpg://imperial_user:${POSTGRES_PASSWORD}@imperial_archives:5432/imperial_db
      - DATABASE_POOL_SIZE=20
      - DATABASE_MAX_OVERFLOW=30
      
      # Cache Configuration
      - REDIS_URL=redis://imperial_cache:6379/0
      - REDIS_PASSWORD=${REDIS_PASSWORD}
      
      # Message Queue Configuration
      - RABBITMQ_URL=amqp://imperial_user:${RABBITMQ_PASSWORD}@imperial_queue:5672/
      
      # Application Configuration
      - APP_NAME=Imperial FastAPI Palace
      - APP_VERSION=${APP_VERSION:-latest}
      - ENVIRONMENT=production
      - DEBUG=false
      - LOG_LEVEL=INFO
      
      # Security Configuration
      - SECRET_KEY=${SECRET_KEY}
      - ALLOWED_HOSTS=*.yourdomain.com,localhost
      - CORS_ORIGINS=https://yourdomain.com,https://www.yourdomain.com
      
      # Monitoring Configuration
      - PROMETHEUS_PORT=9090
      - ENABLE_METRICS=true
      
      # Performance Tuning
      - WORKERS=4
      - MAX_REQUESTS=1000
      - MAX_REQUESTS_JITTER=100
      - TIMEOUT=30
      - KEEPALIVE=2
    volumes:
      # Mount logs for centralized logging
      - ./logs:/app/logs
    networks:
      - imperial-frontend
      - imperial-backend
      - imperial-monitoring
    depends_on:
      imperial_archives:
        condition: service_healthy
      imperial_cache:
        condition: service_healthy
      imperial_queue:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "5"

  fastapi-app-2:
    <<: *fastapi-service
    container_name: imperial_palace_beta

  fastapi-app-3:
    <<: *fastapi-service
    container_name: imperial_palace_gamma

  # ğŸ“š The Imperial Archives - PostgreSQL Database
  imperial_archives:
    image: postgres:15-alpine
    container_name: imperial_archives
    restart: unless-stopped
    environment:
      - POSTGRES_DB=imperial_db
      - POSTGRES_USER=imperial_user
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_INITDB_ARGS=--encoding=UTF8 --locale=en_US.UTF-8
      # Performance tuning for production
      - POSTGRES_SHARED_BUFFERS=256MB
      - POSTGRES_EFFECTIVE_CACHE_SIZE=1GB
      - POSTGRES_WORK_MEM=4MB
      - POSTGRES_MAINTENANCE_WORK_MEM=64MB
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./database/init:/docker-entrypoint-initdb.d:ro
      - ./database/postgresql.conf:/etc/postgresql/postgresql.conf:ro
    networks:
      - imperial-backend
    ports:
      - "5432:5432"  # Only for development; remove in production
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U imperial_user -d imperial_db"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    command: >
      postgres
      -c config_file=/etc/postgresql/postgresql.conf
      -c log_statement=all
      -c log_min_duration_statement=1000
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # âš¡ The Imperial Cache - Redis High-Speed Storage
  imperial_cache:
    image: redis:7-alpine
    container_name: imperial_cache
    restart: unless-stopped
    command: >
      redis-server
      --requirepass ${REDIS_PASSWORD}
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --appendonly yes
      --appendfsync everysec
      --auto-aof-rewrite-percentage 100
      --auto-aof-rewrite-min-size 64mb
    volumes:
      - redis_data:/data
      - ./redis/redis.conf:/usr/local/etc/redis/redis.conf:ro
    networks:
      - imperial-backend
    ports:
      - "6379:6379"  # Only for development
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "3"

  # ğŸ“¬ The Imperial Post - RabbitMQ Message Queue
  imperial_queue:
    image: rabbitmq:3.12-management-alpine
    container_name: imperial_queue
    restart: unless-stopped
    environment:
      - RABBITMQ_DEFAULT_USER=imperial_user
      - RABBITMQ_DEFAULT_PASS=${RABBITMQ_PASSWORD}
      - RABBITMQ_DEFAULT_VHOST=imperial_vhost
      # Performance and reliability settings
      - RABBITMQ_VM_MEMORY_HIGH_WATERMARK=0.8
      - RABBITMQ_DISK_FREE_LIMIT=1GB
    volumes:
      - ./rabbitmq/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf:ro
      - ./rabbitmq/enabled_plugins:/etc/rabbitmq/enabled_plugins:ro
    networks:
      - imperial-backend
      - imperial-monitoring
    ports:
      - "5672:5672"   # AMQP port
      - "15672:15672" # Management UI
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ğŸ“ˆ The Intelligence Hub - Prometheus Monitoring
  prometheus:
    image: prom/prometheus:v2.45.0
    container_name: imperial_intelligence
    restart: unless-stopped
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/rules:/etc/prometheus/rules:ro
      - prometheus_data:/prometheus
    networks:
      - imperial-monitoring
    ports:
      - "9090:9090"
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ğŸ“Š The Imperial Dashboard - Grafana Visualization
  grafana:
    image: grafana/grafana:10.0.0
    container_name: imperial_dashboard
    restart: unless-stopped
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-piechart-panel,grafana-worldmap-panel
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
    networks:
      - imperial-monitoring
      - imperial-frontend
    ports:
      - "3000:3000"
    depends_on:
      - prometheus
    healthcheck:
      test: ["CMD-SHELL", "wget --quiet --tries=1 --spider http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ğŸ“ The Chronicle Keeper - Elasticsearch for Centralized Logging
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.8.0
    container_name: imperial_chronicles
    restart: unless-stopped
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    networks:
      - imperial-monitoring
    ports:
      - "9200:9200"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          memory: 1G
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ğŸ” The Imperial Lens - Kibana for Log Analysis
  kibana:
    image: docker.elastic.co/kibana/kibana:8.8.0
    container_name: imperial_lens
    restart: unless-stopped
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    networks:
      - imperial-monitoring
      - imperial-frontend
    ports:
      - "5601:5601"
    depends_on:
      elasticsearch:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5601/api/status || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: "json-file"
      options:
        max-size: "5m"
        max-file: "3"

# ğŸ” Imperial Secrets Management
# Create a .env file with these variables:
# POSTGRES_PASSWORD=your_secure_postgres_password
# REDIS_PASSWORD=your_secure_redis_password  
# RABBITMQ_PASSWORD=your_secure_rabbitmq_password
# SECRET_KEY=your_very_long_secret_key_for_jwt_tokens
# GRAFANA_PASSWORD=your_grafana_admin_password
# APP_VERSION=1.0.0
```

### ğŸ›¡ï¸ Imperial Security Configuration Files

**nginx/nginx.conf - The Gateway Security System**
```nginx
# nginx.conf - Imperial Gateway Configuration
user nginx;
worker_processes auto;
error_log /var/log/nginx/error.log warn;
pid /var/run/nginx.pid;

events {
    worker_connections 1024;
    use epoll;
    multi_accept on;
}

http {
    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # Security headers
    add_header X-Frame-Options "SAMEORIGIN" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header X-XSS-Protection "1; mode=block" always;
    add_header Referrer-Policy "strict-origin-when-cross-origin" always;
    add_header Content-Security-Policy "default-src 'self'" always;

    # Logging format
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent" "$http_x_forwarded_for" '
                    'rt=$request_time uct="$upstream_connect_time" '
                    'uht="$upstream_header_time" urt="$upstream_response_time"';

    access_log /var/log/nginx/access.log main;

    # Performance optimizations
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    client_max_body_size 20M;

    # Gzip compression
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_proxied any;
    gzip_comp_level 6;
    gzip_types
        text/plain
        text/css
        text/xml
        text/javascript
        application/json
        application/javascript
        application/xml+rss
        application/atom+xml
        image/svg+xml;

    # Rate limiting
    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
    limit_req_zone $binary_remote_addr zone=login:10m rate=1r/s;

    # Upstream backend servers
    upstream fastapi_backend {
        least_conn;
        server fastapi-app-1:8000 max_fails=3 fail_timeout=30s;
        server fastapi-app-2:8000 max_fails=3 fail_timeout=30s;
        server fastapi-app-3:8000 max_fails=3 fail_timeout=30s;
        keepalive 32;
    }

    # Main server configuration
    server {
        listen 80;
        server_name yourdomain.com www.yourdomain.com;
        
        # Security headers
        include /etc/nginx/security-headers.conf;

        # Health check endpoint (bypass rate limiting)
        location /health {
            access_log off;
            proxy_pass http://fastapi_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        # API endpoints with rate limiting
        location /api/ {
            limit_req zone=api burst=20 nodelay;
            limit_req_status 429;
            
            proxy_pass http://fastapi_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # Timeouts
            proxy_connect_timeout 5s;
            proxy_send_timeout 30s;
            proxy_read_timeout 30s;
            
            # Buffer settings
            proxy_buffering on;
            proxy_buffer_size 4k;
            proxy_buffers 8 4k;
        }

        # Login endpoints with stricter rate limiting
        location ~ ^/(auth|login|register) {
            limit_req zone=login burst=5 nodelay;
            
            proxy_pass http://fastapi_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        # Static files and docs
        location ~ ^/(docs|redoc|static)/ {
            proxy_pass http://fastapi_backend;
            proxy_set_header Host $host;
            proxy_cache_valid 200 1h;
            expires 1h;
            add_header Cache-Control "public, immutable";
        }

        # Default location
        location / {
            proxy_pass http://fastapi_backend;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
    }
}
```

### âœ… Imperial Deployment Commands

```bash
# ğŸš€ Launch the Complete Imperial Neighborhood
# This command builds and starts all services in the correct order
docker-compose -f docker-compose.production.yml up -d

# ğŸ“Š Monitor the Imperial Construction Progress
docker-compose -f docker-compose.production.yml logs -f

# ğŸ” Check the Health of All Imperial Buildings
docker-compose -f docker-compose.production.yml ps

# ğŸ“ˆ Scale the Royal Palaces (FastAPI instances)
docker-compose -f docker-compose.production.yml up -d --scale fastapi-app=5

# ğŸ› ï¸ Update a Single Service (Rolling Update)
docker-compose -f docker-compose.production.yml up -d --no-deps fastapi-app-1

# ğŸ”„ Restart All Imperial Services
docker-compose -f docker-compose.production.yml restart

# ğŸ§¹ Clean Shutdown of the Entire Neighborhood
docker-compose -f docker-compose.production.yml down

# ğŸ—‘ï¸ Complete Cleanup (Remove volumes - CAUTION!)
docker-compose -f docker-compose.production.yml down -v --remove-orphans
```

### ğŸ¯ Imperial Monitoring Dashboard

Your neighborhood now includes comprehensive monitoring accessible at:

- **ğŸ¢ Application**: http://localhost:80 (Load balanced across 3 instances)
- **ğŸ“Š Grafana Dashboard**: http://localhost:3000 (admin/your_password)
- **ğŸ“ˆ Prometheus Metrics**: http://localhost:9090
- **ğŸ” Kibana Logs**: http://localhost:5601
- **ğŸ“¬ RabbitMQ Management**: http://localhost:15672 (imperial_user/your_password)

### ğŸ¯ Your Imperial Neighborhood Achievement

ğŸ‰ **Outstanding!** You've constructed a complete imperial neighborhood with:

- âœ… **High Availability**: 3 FastAPI replicas with load balancing
- âœ… **Data Persistence**: PostgreSQL with optimized configuration
- âœ… **Performance Caching**: Redis for high-speed data access
- âœ… **Message Processing**: RabbitMQ for asynchronous operations
- âœ… **Comprehensive Monitoring**: Prometheus + Grafana for observability
- âœ… **Centralized Logging**: ELK stack for log analysis
- âœ… **Security**: Network isolation and proper access controls
- âœ… **Scalability**: Easy horizontal scaling capabilities

**ğŸš€ Next Steps**: Now that you have a neighborhood running smoothly, let's learn how Kubernetes can manage entire cities and global empires.

---

## â˜¸ï¸ Chapter 3: Imperial Administration - Kubernetes Orchestration

### ğŸ¯ The Global Empire Challenge

Docker Compose works brilliantly for neighborhoods, but what happens when your empire spans multiple continents? How do you manage thousands of applications across hundreds of servers? How do you automatically respond to server failures, traffic spikes, and global disasters?

You need **Kubernetes** - the ultimate imperial administration system that can orchestrate container empires at planetary scale.

### ğŸ›ï¸ Understanding the Imperial Hierarchy

```mermaid
graph TD
    subgraph "ğŸŒ Global Kubernetes Empire"
        subgraph "ğŸ›ï¸ Imperial Capital (Control Plane)"
            API[ğŸ‘‘ Imperial API Server<br/>Central Command]
            ETCD[(ğŸ“š Imperial Records<br/>etcd Cluster)]
            SCHED[âš–ï¸ Resource Allocator<br/>Scheduler]
            CTRL[ğŸ›¡ï¸ State Guardian<br/>Controller Manager]
        end
        
        subgraph "ğŸ˜ï¸ Province Alpha (Worker Node)"
            KUBELET1[ğŸ“‹ Provincial Governor<br/>kubelet]
            PROXY1[ğŸš¦ Traffic Controller<br/>kube-proxy]
            subgraph "ğŸ¢ Alpha Districts (Pods)"
                POD1A[ğŸ—ï¸ Palace Service]
                POD1B[ğŸ’¾ Archive Service]
                POD1C[âš¡ Cache Service]
            end
        end
        
        subgraph "ğŸ˜ï¸ Province Beta (Worker Node)"
            KUBELET2[ğŸ“‹ Provincial Governor<br/>kubelet]
            PROXY2[ğŸš¦ Traffic Controller<br/>kube-proxy]
            subgraph "ğŸ¢ Beta Districts (Pods)"
                POD2A[ğŸ—ï¸ Palace Service]
                POD2B[ğŸ“Š Monitor Service]
                POD2C[ğŸ“¬ Message Service]
            end
        end
        
        subgraph "ğŸ˜ï¸ Province Gamma (Worker Node)"
            KUBELET3[ğŸ“‹ Provincial Governor<br/>kubelet]
            PROXY3[ğŸš¦ Traffic Controller<br/>kube-proxy]
            subgraph "ğŸ¢ Gamma Districts (Pods)"
                POD3A[ğŸ—ï¸ Palace Service]
                POD3B[ğŸ’¾ Archive Service]
                POD3C[ğŸ” Search Service]
            end
        end
    end
    
    API --> ETCD
    API --> SCHED
    API --> CTRL
    
    API -.-> KUBELET1
    API -.-> KUBELET2
    API -.-> KUBELET3
    
    KUBELET1 --> POD1A
    KUBELET1 --> POD1B
    KUBELET1 --> POD1C
    
    KUBELET2 --> POD2A
    KUBELET2 --> POD2B
    KUBELET2 --> POD2C
    
    KUBELET3 --> POD3A
    KUBELET3 --> POD3B
    KUBELET3 --> POD3C
    
    style API fill:#e3f2fd,color:#3b3b3b
    style ETCD fill:#bbdefb,color:#3b3b3b
    style POD1A fill:#e8f5e8,color:#3b3b3b
    style POD2A fill:#e8f5e8,color:#3b3b3b
    style POD3A fill:#e8f5e8,color:#3b3b3b
```

### ğŸ­ The Imperial Administration Roles

| ğŸ›ï¸ **Component** | ğŸ¯ **Imperial Analogy** | ğŸ’¼ **Responsibilities** | ğŸ”§ **Technical Function** |
|-------------------|---------------------------|--------------------------|----------------------------|
| **ğŸ‘‘ API Server** | The Emperor's Throne | Central command for all decisions | Validates and processes all API requests |
| **ğŸ“š etcd** | Imperial Library | Stores all empire knowledge | Distributed key-value store for cluster state |
| **âš–ï¸ Scheduler** | Resource Allocator | Decides where to build new structures | Assigns Pods to optimal Nodes |
| **ğŸ›¡ï¸ Controller Manager** | State Guardian | Ensures empire matches desired state | Manages deployments, services, etc. |
| **ğŸ“‹ kubelet** | Provincial Governor | Manages local province affairs | Runs containers and reports node status |
| **ğŸš¦ kube-proxy** | Traffic Director | Routes citizens to correct buildings | Manages network routing and load balancing |

### ğŸ”§ Imperial Manifests: Kubernetes Configuration

Let's transform our Docker Compose neighborhood into Kubernetes manifests:

#### 1. ğŸ“œ Namespace Declaration - Imperial Territory

```yaml
# 00-namespace.yaml - Establish Imperial Territory
apiVersion: v1
kind: Namespace
metadata:
  name: fastapi-empire
  labels:
    name: fastapi-empire
    empire.tier: production
    empire.version: v1
  annotations:
    empire.owner: "imperial-devops@empire.com"
    empire.description: "FastAPI Imperial Application Namespace"
---
# Resource quotas for the empire territory
apiVersion: v1
kind: ResourceQuota
metadata:
  name: empire-resources
  namespace: fastapi-empire
spec:
  hard:
    requests.cpu: "10"      # 10 CPU cores reserved
    requests.memory: 20Gi   # 20GB memory reserved
    limits.cpu: "20"        # 20 CPU cores maximum
    limits.memory: 40Gi     # 40GB memory maximum
    persistentvolumeclaims: "10"  # Maximum persistent volumes
    pods: "50"              # Maximum pods in namespace
    services: "20"          # Maximum services
---
# Network policies for security
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: empire-network-policy
  namespace: fastapi-empire
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: fastapi-empire
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          name: fastapi-empire
  - to: []
    ports:
    - protocol: TCP
      port: 53
    - protocol: UDP
      port: 53
```

#### 2. ğŸ—„ï¸ Imperial Secrets & Configuration

```yaml
# 01-secrets.yaml - Imperial Vault (Create secrets separately for security)
apiVersion: v1
kind: Secret
metadata:
  name: imperial-secrets
  namespace: fastapi-empire
  labels:
    app: fastapi-empire
    component: configuration
type: Opaque
data:
  # Base64 encoded secrets (use kubectl create secret or external secret management)
  postgres-password: <base64-encoded-password>
  redis-password: <base64-encoded-password>
  secret-key: <base64-encoded-jwt-secret>
  
---
# Configuration that's not secret
apiVersion: v1
kind: ConfigMap
metadata:
  name: imperial-config
  namespace: fastapi-empire
  labels:
    app: fastapi-empire
    component: configuration
data:
  # Database configuration
  POSTGRES_DB: "imperial_db"
  POSTGRES_USER: "imperial_user"
  DATABASE_POOL_SIZE: "20"
  DATABASE_MAX_OVERFLOW: "30"
  
  # Application configuration
  APP_NAME: "FastAPI Imperial Palace"
  ENVIRONMENT: "production"
  LOG_LEVEL: "INFO"
  DEBUG: "false"
  
  # Performance settings
  WORKERS: "4"
  MAX_REQUESTS: "1000"
  TIMEOUT: "30"
  
  # Monitoring configuration
  ENABLE_METRICS: "true"
  PROMETHEUS_PORT: "8080"
  
  # Redis configuration
  REDIS_DB: "0"
  REDIS_MAX_CONNECTIONS: "100"
  
---
# Advanced configuration for different environments
apiVersion: v1
kind: ConfigMap
metadata:
  name: imperial-nginx-config
  namespace: fastapi-empire
data:
  nginx.conf: |
    events {
      worker_connections 1024;
    }
    
    http {
      upstream fastapi_backend {
        server imperial-palace-service:8000;
      }
      
      server {
        listen 80;
        location /health {
          access_log off;
          proxy_pass http://fastapi_backend;
        }
        
        location / {
          proxy_pass http://fastapi_backend;
          proxy_set_header Host $host;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
          proxy_set_header X-Forwarded-Proto $scheme;
        }
      }
    }
```

#### 3. ğŸ¢ Imperial Palace Deployment

```yaml
# 02-fastapi-deployment.yaml - The Main Imperial Palace
apiVersion: apps/v1
kind: Deployment
metadata:
  name: imperial-palace
  namespace: fastapi-empire
  labels:
    app: imperial-palace
    component: api
    version: v1
  annotations:
    deployment.kubernetes.io/revision: "1"
    empire.deployment.strategy: "RollingUpdate"
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1        # Allow 1 extra pod during updates
      maxUnavailable: 0  # Ensure zero downtime
  selector:
    matchLabels:
      app: imperial-palace
      component: api
  template:
    metadata:
      labels:
        app: imperial-palace
        component: api
        version: v1
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      # Security context for the entire pod
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
      
      # Service account for RBAC
      serviceAccountName: imperial-palace-sa
      
      # Ensure pods are distributed across nodes
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - imperial-palace
              topologyKey: kubernetes.io/hostname
      
      # Graceful shutdown configuration
      terminationGracePeriodSeconds: 30
      
      # DNS configuration for service discovery
      dnsPolicy: ClusterFirst
      dnsConfig:
        options:
        - name: ndots
          value: "2"
        - name: edns0
      
      containers:
      - name: fastapi-app
        image: your-registry.com/imperial-palace:latest
        imagePullPolicy: Always
        
        # Container security
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
        
        ports:
        - name: http
          containerPort: 8000
          protocol: TCP
        - name: metrics
          containerPort: 8080
          protocol: TCP
        
        # Environment variables from ConfigMap and Secrets
        envFrom:
        - configMapRef:
            name: imperial-config
        env:
        - name: DATABASE_URL
          value: "postgresql+asyncpg://$(POSTGRES_USER):$(POSTGRES_PASSWORD)@imperial-archives:5432/$(POSTGRES_DB)"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: imperial-secrets
              key: postgres-password
        - name: REDIS_URL
          value: "redis://:$(REDIS_PASSWORD)@imperial-cache:6379/$(REDIS_DB)"
        - name: REDIS_PASSWORD
          valueFrom:
            secretKeyRef:
              name: imperial-secrets
              key: redis-password
        - name: SECRET_KEY
          valueFrom:
            secretKeyRef:
              name: imperial-secrets
              key: secret-key
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        
        # Resource management
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        
        # Health checks for Kubernetes orchestration
        livenessProbe:
          httpGet:
            path: /health/live
            port: http
            scheme: HTTP
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /health/ready
            port: http
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          successThreshold: 1
          failureThreshold: 3
        
        # Startup probe for slow-starting applications
        startupProbe:
          httpGet:
            path: /health/live
            port: http
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 30  # Allow up to 5 minutes for startup
        
        # Volume mounts for writable directories
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: cache
          mountPath: /app/cache
        - name: logs
          mountPath: /app/logs
      
      # Init containers for setup tasks
      initContainers:
      - name: migration
        image: your-registry.com/imperial-palace:latest
        command: ["python", "-m", "alembic", "upgrade", "head"]
        envFrom:
        - configMapRef:
            name: imperial-config
        env:
        - name: DATABASE_URL
          value: "postgresql+asyncpg://$(POSTGRES_USER):$(POSTGRES_PASSWORD)@imperial-archives:5432/$(POSTGRES_DB)"
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: imperial-secrets
              key: postgres-password
      
      # Volumes for writable filesystems
      volumes:
      - name: tmp
        emptyDir: {}
      - name: cache
        emptyDir:
          sizeLimit: 1Gi
      - name: logs
        emptyDir:
          sizeLimit: 2Gi
      
      # Image pull secrets if using private registry
      imagePullSecrets:
      - name: imperial-registry-secret

---
# Service Account for RBAC
apiVersion: v1
kind: ServiceAccount
metadata:
  name: imperial-palace-sa
  namespace: fastapi-empire
  labels:
    app: imperial-palace
    component: rbac

---
# Horizontal Pod Autoscaler
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: imperial-palace-hpa
  namespace: fastapi-empire
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: imperial-palace
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 30
      policies:
      - type: Percent
        value: 50
        periodSeconds: 30
```

#### 4. ğŸŒ Imperial Services & Networking

```yaml
# 03-services.yaml - Imperial Communication Network
apiVersion: v1
kind: Service
metadata:
  name: imperial-palace-service
  namespace: fastapi-empire
  labels:
    app: imperial-palace
    component: api
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: nlb
    prometheus.io/scrape: "true"
    prometheus.io/port: "8080"
spec:
  type: ClusterIP  # Use ClusterIP for internal communication
  selector:
    app: imperial-palace
    component: api
  ports:
  - name: http
    port: 8000
    targetPort: http
    protocol: TCP
  - name: metrics
    port: 8080
    targetPort: metrics
    protocol: TCP
  sessionAffinity: None

---
# Ingress for external access
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: imperial-palace-ingress
  namespace: fastapi-empire
  labels:
    app: imperial-palace
  annotations:
    # Nginx ingress controller annotations
    kubernetes.io/ingress.class: "nginx"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
    
    # Rate limiting
    nginx.ingress.kubernetes.io/rate-limit: "100"
    nginx.ingress.kubernetes.io/rate-limit-window: "1m"
    
    # Security headers
    nginx.ingress.kubernetes.io/server-snippet: |
      add_header X-Frame-Options "SAMEORIGIN" always;
      add_header X-Content-Type-Options "nosniff" always;
      add_header X-XSS-Protection "1; mode=block" always;
      add_header Referrer-Policy "strict-origin-when-cross-origin" always;
    
    # CORS configuration
    nginx.ingress.kubernetes.io/enable-cors: "true"
    nginx.ingress.kubernetes.io/cors-allow-origin: "https://yourdomain.com,https://www.yourdomain.com"
    nginx.ingress.kubernetes.io/cors-allow-methods: "GET, POST, PUT, DELETE, OPTIONS"
    nginx.ingress.kubernetes.io/cors-allow-headers: "DNT,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Range,Authorization"
    
    # SSL/TLS configuration
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/ssl-protocols: "TLSv1.2 TLSv1.3"
    nginx.ingress.kubernetes.io/ssl-ciphers: "ECDHE-ECDSA-AES256-GCM-SHA384,ECDHE-RSA-AES256-GCM-SHA384"
    
    # Performance optimizations
    nginx.ingress.kubernetes.io/proxy-body-size: "20m"
    nginx.ingress.kubernetes.io/proxy-connect-timeout: "5"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "30"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "30"
    
spec:
  tls:
  - hosts:
    - api.yourdomain.com
    - www.api.yourdomain.com
    secretName: imperial-palace-tls
  rules:
  - host: api.yourdomain.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: imperial-palace-service
            port:
              number: 8000
  - host: www.api.yourdomain.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: imperial-palace-service
            port:
              number: 8000

---
# Service Monitor for Prometheus
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: imperial-palace-metrics
  namespace: fastapi-empire
  labels:
    app: imperial-palace
    component: monitoring
spec:
  selector:
    matchLabels:
      app: imperial-palace
      component: api
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics
    scheme: http
```

### ğŸ¯ Imperial Database Configuration

```yaml
# 04-postgresql.yaml - Imperial Archives (Database)
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: imperial-archives-pvc
  namespace: fastapi-empire
  labels:
    app: imperial-archives
    component: database
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: fast-ssd  # Use appropriate storage class
  resources:
    requests:
      storage: 100Gi

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: imperial-archives
  namespace: fastapi-empire
  labels:
    app: imperial-archives
    component: database
spec:
  serviceName: imperial-archives
  replicas: 1  # Start with single instance; clustering comes later
  selector:
    matchLabels:
      app: imperial-archives
      component: database
  template:
    metadata:
      labels:
        app: imperial-archives
        component: database
    spec:
      securityContext:
        runAsUser: 999
        runAsGroup: 999
        fsGroup: 999
      containers:
      - name: postgresql
        image: postgres:15-alpine
        imagePullPolicy: IfNotPresent
        
        env:
        - name: POSTGRES_DB
          valueFrom:
            configMapKeyRef:
              name: imperial-config
              key: POSTGRES_DB
        - name: POSTGRES_USER
          valueFrom:
            configMapKeyRef:
              name: imperial-config
              key: POSTGRES_USER
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: imperial-secrets
              key: postgres-password
        - name: PGDATA
          value: /var/lib/postgresql/data/pgdata
        
        ports:
        - name: postgresql
          containerPort: 5432
          protocol: TCP
        
        # Enhanced resource allocation for production
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        
        # PostgreSQL health checks
        livenessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - exec pg_isready -U "$POSTGRES_USER" -d "$POSTGRES_DB" -h 127.0.0.1 -p 5432
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 6
        
        readinessProbe:
          exec:
            command:
            - /bin/sh
            - -c
            - exec pg_isready -U "$POSTGRES_USER" -d "$POSTGRES_DB" -h 127.0.0.1 -p 5432
          initialDelaySeconds: 5
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        
        volumeMounts:
        - name: postgresql-data
          mountPath: /var/lib/postgresql/data
        - name: postgresql-config
          mountPath: /etc/postgresql/postgresql.conf
          subPath: postgresql.conf
          readOnly: true
        
      volumes:
      - name: postgresql-config
        configMap:
          name: postgresql-config
  
  volumeClaimTemplates:
  - metadata:
      name: postgresql-data
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: fast-ssd
      resources:
        requests:
          storage: 100Gi

---
# PostgreSQL Service
apiVersion: v1
kind: Service
metadata:
  name: imperial-archives
  namespace: fastapi-empire
  labels:
    app: imperial-archives
    component: database
spec:
  type: ClusterIP
  selector:
    app: imperial-archives
    component: database
  ports:
  - name: postgresql
    port: 5432
    targetPort: postgresql
    protocol: TCP

---
# PostgreSQL Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgresql-config
  namespace: fastapi-empire
data:
  postgresql.conf: |
    # Memory settings
    shared_buffers = 256MB
    effective_cache_size = 1GB
    work_mem = 4MB
    maintenance_work_mem = 64MB
    
    # Checkpoint settings
    checkpoint_completion_target = 0.7
    wal_buffers = 7864kB
    default_statistics_target = 100
    
    # Connection settings
    max_connections = 200
    
    # Logging
    log_statement = 'all'
    log_min_duration_statement = 1000
    log_checkpoints = on
    log_connections = on
    log_disconnections = on
    log_lock_waits = on
    
    # Performance
    random_page_cost = 1.1
    effective_io_concurrency = 200
```

### âœ… Imperial Deployment Commands

```bash
# ğŸ¯ Deploy the Complete Imperial Infrastructure

# 1. Create the imperial namespace and configuration
kubectl apply -f 00-namespace.yaml
kubectl apply -f 01-secrets.yaml

# 2. Deploy the database infrastructure
kubectl apply -f 04-postgresql.yaml

# 3. Wait for database to be ready
kubectl wait --for=condition=ready pod -l app=imperial-archives -n fastapi-empire --timeout=300s

# 4. Deploy the main application
kubectl apply -f 02-fastapi-deployment.yaml

# 5. Set up networking and ingress
kubectl apply -f 03-services.yaml

# ğŸ” Monitor the deployment progress
watch kubectl get pods -n fastapi-empire

# ğŸ“Š Check deployment status
kubectl get deployments,statefulsets,services,ingress -n fastapi-empire

# ğŸ“ View application logs
kubectl logs -f deployment/imperial-palace -n fastapi-empire

# ğŸ”§ Scale the application
kubectl scale deployment imperial-palace --replicas=5 -n fastapi-empire

# ğŸš€ Rolling update with new image
kubectl set image deployment/imperial-palace fastapi-app=your-registry.com/imperial-palace:v2.0.0 -n fastapi-empire

# ğŸ“ˆ Check horizontal pod autoscaler
kubectl get hpa -n fastapi-empire

# ğŸ¯ Get service endpoints
kubectl get endpoints -n fastapi-empire

# ğŸ” Debug pod issues
kubectl describe pod <pod-name> -n fastapi-empire
kubectl exec -it <pod-name> -n fastapi-empire -- /bin/bash

# ğŸ“Š Monitor resource usage
kubectl top pods -n fastapi-empire
kubectl top nodes
```

### ğŸ¯ Your Imperial Kubernetes Achievement

ğŸ‰ **Extraordinary!** You've successfully deployed a complete Kubernetes empire with:

- âœ… **Auto-scaling**: Horizontal Pod Autoscaler responds to traffic
- âœ… **Zero-downtime Deployment**: Rolling updates with health checks
- âœ… **High Availability**: Pod anti-affinity across nodes
- âœ… **Security**: RBAC, network policies, and non-root containers
- âœ… **Observability**: Prometheus metrics and health monitoring
- âœ… **Resource Management**: Quotas, limits, and requests
- âœ… **Professional Networking**: Ingress with SSL termination

**ğŸš€ Next Steps**: Let's add enterprise-grade security, monitoring, and CI/CD automation to your empire.

---

## ğŸ›¡ï¸ Chapter 4: Imperial Security & Compliance

### ğŸ¯ The Zero-Trust Empire

In the digital age, even emperors must assume that enemies could be anywhereâ€”inside the palace walls, among trusted advisors, or infiltrating supply lines. **Zero-Trust Security** means verifying every citizen, every communication, and every action, regardless of their position or past loyalty.

### ğŸ° The Imperial Security Framework

```mermaid
graph TD
    subgraph "ğŸ›¡ï¸ Imperial Security Layers"
        subgraph "ğŸŒ Perimeter Defense"
            WAF[ğŸ”¥ Web Application Firewall<br/>First Line of Defense]
            LB[âš–ï¸ Load Balancer<br/>Traffic Inspection]
            DDOS[ğŸ›¡ï¸ DDoS Protection<br/>Attack Mitigation]
        end
        
        subgraph "ğŸ” Identity & Access"
            IAM[ğŸ‘‘ Identity Management<br/>Royal Authentication]
            RBAC[ğŸ“œ Role-Based Access<br/>Permission Hierarchy]
            MFA[ğŸ”‘ Multi-Factor Auth<br/>Dual Verification]
        end
        
        subgraph "ğŸ•µï¸ Runtime Security"
            SCAN[ğŸ” Vulnerability Scanner<br/>Security Audits]
            POLICY[ğŸ“‹ Security Policies<br/>Automated Enforcement]
            MONITOR[ğŸ‘ï¸ Threat Detection<br/>Continuous Surveillance]
        end
        
        subgraph "ğŸ“Š Data Protection"
            ENCRYPT[ğŸ”’ Encryption at Rest<br/>Data Vault Security]
            TLS[ğŸŒ TLS in Transit<br/>Communication Security]
            BACKUP[ğŸ’¿ Secure Backups<br/>Disaster Recovery]
        end
        
        subgraph "ğŸ“ Compliance & Audit"
            LOGS[ğŸ“š Audit Logs<br/>Historical Evidence]
            COMPLIANCE[âœ… Standards Compliance<br/>Regulatory Requirements]
            FORENSICS[ğŸ•µï¸ Digital Forensics<br/>Incident Investigation]
        end
    end
    
    WAF --> LB
    LB --> DDOS
    IAM --> RBAC
    RBAC --> MFA
    SCAN --> POLICY
    POLICY --> MONITOR
    ENCRYPT --> TLS
    TLS --> BACKUP
    LOGS --> COMPLIANCE
    COMPLIANCE --> FORENSICS
    
    style WAF fill:#ffebee,color:#3b3b3b
    style IAM fill:#e8f5e8,color:#3b3b3b
    style SCAN fill:#fff3e0,color:#3b3b3b
    style ENCRYPT fill:#e3f2fd,color:#3b3b3b
    style LOGS fill:#f3e5f5,color:#3b3b3b
```

### ğŸ”§ Imperial Security Implementation

#### 1. ğŸ” Advanced Secret Management

```yaml
# 05-security-secrets.yaml - Imperial Vault System
apiVersion: v1
kind: Secret
metadata:
  name: imperial-vault
  namespace: fastapi-empire
  labels:
    security.level: maximum
    vault.type: imperial
type: Opaque
data:
  # All secrets are encrypted and rotated regularly
  database-master-key: <base64-encoded-master-key>
  jwt-signing-key: <base64-encoded-jwt-key>
  encryption-salt: <base64-encoded-salt>
  api-keys: <base64-encoded-api-keys>

---
# External Secrets Operator for enterprise secret management
apiVersion: external-secrets.io/v1beta1
kind: SecretStore
metadata:
  name: imperial-vault-store
  namespace: fastapi-empire
spec:
  provider:
    vault:
      server: "https://vault.empire.internal"
      path: "secret"
      version: "v2"
      auth:
        kubernetes:
          mountPath: "kubernetes"
          role: "imperial-palace"

---
apiVersion: external-secrets.io/v1beta1
kind: ExternalSecret
metadata:
  name: imperial-external-secrets
  namespace: fastapi-empire
spec:
  refreshInterval: 15s
  secretStoreRef:
    name: imperial-vault-store
    kind: SecretStore
  target:
    name: imperial-secrets
    creationPolicy: Owner
  data:
  - secretKey: postgres-password
    remoteRef:
      key: database/postgresql
      property: password
  - secretKey: jwt-secret
    remoteRef:
      key: auth/jwt
      property: secret
```

#### 2. ğŸ›¡ï¸ Network Security Policies

```yaml
# 06-network-security.yaml - Imperial Network Fortress
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: imperial-fortress-policy
  namespace: fastapi-empire
  labels:
    security.level: maximum
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  
  # Ingress rules - who can enter the empire
  ingress:
  # Allow traffic from ingress controllers
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 8000
  
  # Allow internal communication within the empire
  - from:
    - podSelector:
        matchLabels:
          empire.security.clearance: high
    ports:
    - protocol: TCP
      port: 8000
    - protocol: TCP
      port: 8080
  
  # Egress rules - where the empire can communicate
  egress:
  # DNS resolution
  - to: []
    ports:
    - protocol: UDP
      port: 53
    - protocol: TCP
      port: 53
  
  # HTTPS to external services
  - to: []
    ports:
    - protocol: TCP
      port: 443
  
  # Internal communication to database
  - to:
    - podSelector:
        matchLabels:
          app: imperial-archives
    ports:
    - protocol: TCP
      port: 5432
  
  # Internal communication to cache
  - to:
    - podSelector:
        matchLabels:
          app: imperial-cache
    ports:
    - protocol: TCP
      port: 6379

---
# Pod Security Policy for enhanced security
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: imperial-security-policy
  namespace: fastapi-empire
  labels:
    security.level: maximum
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'
```

#### 3. ğŸ” Security Monitoring & Scanning

```yaml
# 07-security-monitoring.yaml - Imperial Intelligence Network
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: imperial-security-scanner
  namespace: fastapi-empire
  labels:
    app: security-scanner
    component: monitoring
spec:
  selector:
    matchLabels:
      app: security-scanner
  template:
    metadata:
      labels:
        app: security-scanner
        security.level: maximum
    spec:
      serviceAccount: security-scanner-sa
      containers:
      - name: falco
        image: falcosecurity/falco:0.35.1
        args:
          - /usr/bin/falco
          - --cri=/run/containerd/containerd.sock
          - --k8s-api=https://kubernetes.default.svc.cluster.local:443
          - --k8s-api-cert=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          - --k8s-api-key=/var/run/secrets/kubernetes.io/serviceaccount/token
        securityContext:
          privileged: true
        resources:
          limits:
            memory: 512Mi
            cpu: 200m
          requests:
            memory: 256Mi
            cpu: 100m
        volumeMounts:
        - mountPath: /host/var/run/docker.sock
          name: docker-socket
        - mountPath: /host/dev
          name: dev-fs
          readOnly: true
        - mountPath: /host/proc
          name: proc-fs
          readOnly: true
        - mountPath: /host/boot
          name: boot-fs
          readOnly: true
        - mountPath: /host/lib/modules
          name: lib-modules
        - mountPath: /host/usr
          name: usr-fs
          readOnly: true
        - mountPath: /host/etc
          name: etc-fs
          readOnly: true
      
      volumes:
      - name: docker-socket
        hostPath:
          path: /var/run/docker.sock
      - name: dev-fs
        hostPath:
          path: /dev
      - name: proc-fs
        hostPath:
          path: /proc
      - name: boot-fs
        hostPath:
          path: /boot
      - name: lib-modules
        hostPath:
          path: /lib/modules
      - name: usr-fs
        hostPath:
          path: /usr
      - name: etc-fs
        hostPath:
          path: /etc
      
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master

---
# Service Account for Security Scanner
apiVersion: v1
kind: ServiceAccount
metadata:
  name: security-scanner-sa
  namespace: fastapi-empire

---
# ClusterRole for Security Scanner
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: security-scanner-role
rules:
- apiGroups: [""]
  resources: ["nodes", "namespaces", "pods", "replicationcontrollers", "services"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["apps"]
  resources: ["daemonsets", "deployments", "replicasets", "statefulsets"]
  verbs: ["get", "list", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: security-scanner-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: security-scanner-role
subjects:
- kind: ServiceAccount
  name: security-scanner-sa
  namespace: fastapi-empire
```

---

## ğŸ”§ Chapter 5: Enterprise CI/CD Pipeline

### ğŸ¯ The Imperial Assembly Line

Think of CI/CD as the imperial assembly line that transforms raw ideas (code) into deployed empires. Every change goes through rigorous quality control, testing, and approval before reaching citizens.

### ğŸ—ï¸ The Complete Imperial Pipeline

```mermaid
graph TD
    subgraph "ğŸ‘¨â€ğŸ’» Development Phase"
        CODE[ğŸ“ Developer Commits<br/>Code Changes]
        PR[ğŸ”„ Pull Request<br/>Peer Review]
        MERGE[âœ… Merge to Main<br/>Quality Gate]
    end
    
    subgraph "ğŸ§ª Testing Empire"
        UNIT[âš¡ Unit Tests<br/>Individual Component Testing]
        INT[ğŸ”— Integration Tests<br/>System Interaction Testing]
        SEC[ğŸ›¡ï¸ Security Scans<br/>Vulnerability Assessment]
        PERF[ğŸ“Š Performance Tests<br/>Load & Stress Testing]
    end
    
    subgraph "ğŸ—ï¸ Build Empire"
        BUILD[ğŸ“¦ Container Build<br/>Docker Image Creation]
        SCAN[ğŸ” Image Scanning<br/>Security Validation]
        PUSH[ğŸ“¤ Registry Push<br/>Artifact Storage]
        SIGN[ğŸ” Image Signing<br/>Supply Chain Security]
    end
    
    subgraph "ğŸš€ Deployment Empire"
        STAGE[ğŸ­ Staging Deploy<br/>Pre-production Testing]
        SMOKE[ğŸ’¨ Smoke Tests<br/>Basic Functionality]
        CANARY[ğŸ¤ Canary Deploy<br/>Gradual Rollout]
        PROD[ğŸŒ Production Deploy<br/>Full Rollout]
    end
    
    subgraph "ğŸ“Š Monitoring Empire"
        HEALTH[â¤ï¸ Health Checks<br/>System Vitals]
        METRICS[ğŸ“ˆ Performance Metrics<br/>Business KPIs]
        ALERTS[ğŸš¨ Alert System<br/>Issue Detection]
        ROLLBACK[ğŸ”„ Auto Rollback<br/>Failure Recovery]
    end
    
    CODE --> PR
    PR --> MERGE
    MERGE --> UNIT
    UNIT --> INT
    INT --> SEC
    SEC --> PERF
    PERF --> BUILD
    BUILD --> SCAN
    SCAN --> PUSH
    PUSH --> SIGN
    SIGN --> STAGE
    STAGE --> SMOKE
    SMOKE --> CANARY
    CANARY --> PROD
    PROD --> HEALTH
    HEALTH --> METRICS
    METRICS --> ALERTS
    ALERTS --> ROLLBACK
    
    style CODE fill:#e8f5e8,color:#3b3b3b
    style BUILD fill:#fff3e0,color:#3b3b3b
    style STAGE fill:#e3f2fd,color:#3b3b3b
    style PROD fill:#ffebee,color:#3b3b3b
    style HEALTH fill:#f3e5f5,color:#3b3b3b
```

### ğŸ”§ GitHub Actions Imperial Workflow

**.github/workflows/imperial-cicd.yml - The Complete Pipeline**

```yaml
name: ğŸ›ï¸ Imperial CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: imperial-palace
  REGISTRY_URL: ghcr.io/${{ github.repository }}

jobs:
  # ğŸ§ª Quality Assurance Empire
  quality-gates:
    name: ğŸ›¡ï¸ Imperial Quality Gates
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11]
    
    steps:
    - name: ğŸ“¥ Checkout Imperial Code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: ğŸ Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: ğŸ“¦ Cache Imperial Dependencies
      uses: actions/cache@v3
      with:
        path: |
          ~/.cache/pip
          ~/.cache/pre-commit
        key: ${{ runner.os }}-imperial-${{ matrix.python-version }}-${{ hashFiles('**/requirements*.txt') }}

    - name: ğŸ› ï¸ Install Imperial Tools
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt

    - name: ğŸ” Imperial Code Formatting Check
      run: |
        echo "ğŸ¨ Checking code formatting with Black..."
        black --check --diff .
        
        echo "ğŸ“‹ Checking import organization with isort..."
        isort --check-only --diff .

    - name: ğŸ”¬ Imperial Code Quality Analysis
      run: |
        echo "ğŸ” Running linting with flake8..."
        flake8 app/ tests/
        
        echo "ğŸ”’ Running type checking with mypy..."
        mypy app/
        
        echo "ğŸ›¡ï¸ Running security scan with bandit..."
        bandit -r app/ -f json -o bandit-report.json || true

    - name: ğŸ§ª Imperial Unit Testing
      run: |
        echo "âš¡ Running unit tests with coverage..."
        pytest tests/unit/ -v \
          --cov=app \
          --cov-report=xml \
          --cov-report=html \
          --cov-report=term-missing \
          --junitxml=pytest-unit.xml

    - name: ğŸ”— Imperial Integration Testing
      run: |
        echo "ğŸ”— Running integration tests..."
        pytest tests/integration/ -v \
          --junitxml=pytest-integration.xml
      env:
        DATABASE_URL: postgresql://test:test@localhost:5432/test_db
        REDIS_URL: redis://localhost:6379

    - name: ğŸ›¡ï¸ Imperial Security Scanning
      run: |
        echo "ğŸ” Running dependency vulnerability scan..."
        safety check --json --output safety-report.json || true
        
        echo "ğŸ“‹ Running SAST analysis..."
        semgrep --config=auto --json --output=semgrep-report.json app/ || true

    - name: ğŸ“Š Upload Coverage to Imperial Archives
      uses: codecov/codecov-action@v3
      with:
        files: ./coverage.xml
        fail_ci_if_error: true
        verbose: true

    - name: ğŸ“‹ Upload Test Results
      uses: dorny/test-reporter@v1
      if: success() || failure()
      with:
        name: Imperial Test Results (Python ${{ matrix.python-version }})
        path: pytest-*.xml
        reporter: java-junit

  # ğŸ—ï¸ Imperial Container Construction
  build-imperial-image:
    name: ğŸ—ï¸ Build Imperial Container
    needs: quality-gates
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
    
    outputs:
      image-tag: ${{ steps.meta.outputs.tags }}
      image-digest: ${{ steps.build.outputs.digest }}

    steps:
    - name: ğŸ“¥ Checkout Imperial Blueprints
      uses: actions/checkout@v4

    - name: ğŸ³ Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: ğŸ” Login to Imperial Registry
      uses: docker/login-action@v3
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}

    - name: ğŸ“‹ Extract Imperial Metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY_URL }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix=sha-
          type=raw,value=latest,enable={{is_default_branch}}

    - name: ğŸ—ï¸ Build and Push Imperial Image
      id: build
      uses: docker/build-push-action@v5
      with:
        context: .
        platforms: linux/amd64,linux/arm64
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max
        build-args: |
          BUILD_DATE=${{ github.event.head_commit.timestamp }}
          VCS_REF=${{ github.sha }}
          VERSION=${{ steps.meta.outputs.version }}

    - name: ğŸ” Scan Imperial Image for Vulnerabilities
      uses: aquasecurity/trivy-action@master
      with:
        image-ref: ${{ env.REGISTRY_URL }}:${{ steps.meta.outputs.version }}
        format: 'sarif'
        output: 'trivy-results.sarif'

    - name: ğŸ“‹ Upload Vulnerability Scan Results
      uses: github/codeql-action/upload-sarif@v2
      if: always()
      with:
        sarif_file: 'trivy-results.sarif'

    - name: ğŸ” Sign Imperial Image
      uses: sigstore/cosign-installer@v3
    
    - name: ğŸ” Sign Container Image
      run: |
        echo "${{ secrets.COSIGN_PRIVATE_KEY }}" > cosign.key
        cosign sign --key cosign.key ${{ env.REGISTRY_URL }}@${{ steps.build.outputs.digest }}
      env:
        COSIGN_PASSWORD: ${{ secrets.COSIGN_PASSWORD }}

  # ğŸ­ Imperial Staging Deployment
  deploy-staging:
    name: ğŸ­ Deploy to Imperial Staging
    needs: build-imperial-image
    runs-on: ubuntu-latest
    environment: staging
    
    steps:
    - name: ğŸ“¥ Checkout Deployment Manifests
      uses: actions/checkout@v4

    - name: âš™ï¸ Configure Imperial Kubernetes
      uses: azure/k8s-set-context@v3
      with:
        method: kubeconfig
        kubeconfig: ${{ secrets.KUBECONFIG_STAGING }}

    - name: ğŸ”„ Deploy to Staging Empire
      run: |
        # Update image tag in manifests
        sed -i "s|image: .*|image: ${{ needs.build-imperial-image.outputs.image-tag }}|g" k8s/02-fastapi-deployment.yaml
        
        # Apply staging configuration
        kubectl apply -f k8s/00-namespace.yaml
        kubectl apply -f k8s/01-secrets.yaml
        kubectl apply -f k8s/02-fastapi-deployment.yaml
        kubectl apply -f k8s/03-services.yaml
        
        # Wait for deployment to complete
        kubectl rollout status deployment/imperial-palace -n fastapi-empire --timeout=600s

    - name: ğŸ’¨ Imperial Smoke Tests
      run: |
        # Get staging URL
        STAGING_URL=$(kubectl get ingress imperial-palace-ingress -n fastapi-empire -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
        
        # Wait for service to be ready
        echo "ğŸ¥ Waiting for health checks..."
        for i in {1..30}; do
          if curl -f "https://${STAGING_URL}/health/live"; then
            echo "âœ… Staging empire is alive!"
            break
          fi
          echo "â³ Waiting for staging empire... (attempt $i/30)"
          sleep 10
        done
        
        # Run comprehensive smoke tests
        echo "ğŸ’¨ Running smoke tests..."
        pytest tests/smoke/ --base-url="https://${STAGING_URL}" -v

  # ğŸ¤ Imperial Canary Deployment
  deploy-canary:
    name: ğŸ¤ Canary Deployment
    needs: [build-imperial-image, deploy-staging]
    runs-on: ubuntu-latest
    environment: production
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: ğŸ“¥ Checkout Deployment Manifests
      uses: actions/checkout@v4

    - name: âš™ï¸ Configure Production Kubernetes
      uses: azure/k8s-set-context@v3
      with:
        method: kubeconfig
        kubeconfig: ${{ secrets.KUBECONFIG_PRODUCTION }}

    - name: ğŸ¤ Deploy Canary Version
      run: |
        # Create canary deployment (10% traffic)
        sed "s|name: imperial-palace|name: imperial-palace-canary|g" k8s/02-fastapi-deployment.yaml > canary-deployment.yaml
        sed -i "s|replicas: 3|replicas: 1|g" canary-deployment.yaml
        sed -i "s|image: .*|image: ${{ needs.build-imperial-image.outputs.image-tag }}|g" canary-deployment.yaml
        
        kubectl apply -f canary-deployment.yaml
        kubectl rollout status deployment/imperial-palace-canary -n fastapi-empire --timeout=600s

    - name: ğŸ“Š Monitor Canary Metrics
      run: |
        echo "ğŸ“Š Monitoring canary deployment for 5 minutes..."
        
        # Monitor error rates, response times, and success rates
        for i in {1..30}; do
          ERROR_RATE=$(kubectl exec -n fastapi-empire deployment/prometheus -- \
            promtool query instant 'rate(http_requests_total{status=~"5.."}[5m])' | \
            grep -o '[0-9.]*' | head -1)
          
          if (( $(echo "$ERROR_RATE > 0.05" | bc -l) )); then
            echo "ğŸš¨ High error rate detected: $ERROR_RATE - Rolling back!"
            exit 1
          fi
          
          echo "âœ… Canary healthy - Error rate: $ERROR_RATE (attempt $i/30)"
          sleep 10
        done

    - name: âœ… Promote Canary to Production
      run: |
        echo "ğŸš€ Promoting canary to full production..."
        
        # Update main deployment with new image
        sed -i "s|image: .*|image: ${{ needs.build-imperial-image.outputs.image-tag }}|g" k8s/02-fastapi-deployment.yaml
        kubectl apply -f k8s/02-fastapi-deployment.yaml
        
        # Wait for rolling update
        kubectl rollout status deployment/imperial-palace -n fastapi-empire --timeout=600s
        
        # Remove canary deployment
        kubectl delete deployment imperial-palace-canary -n fastapi-empire

  # ğŸ“Š Imperial Production Monitoring
  monitor-production:
    name: ğŸ“Š Monitor Imperial Production
    needs: deploy-canary
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
    - name: ğŸ“ˆ Verify Production Health
      run: |
        echo "ğŸ¥ Verifying production empire health..."
        
        # Get production URL
        PROD_URL="https://api.yourdomain.com"
        
        # Comprehensive health check
        curl -f "${PROD_URL}/health/detailed" | jq '.'
        
        # Performance verification
        RESPONSE_TIME=$(curl -o /dev/null -s -w "%{time_total}" "${PROD_URL}/health/live")
        echo "âš¡ Response time: ${RESPONSE_TIME}s"
        
        if (( $(echo "$RESPONSE_TIME > 2.0" | bc -l) )); then
          echo "ğŸŒ Slow response time detected!"
          exit 1
        fi

    - name: ğŸš¨ Setup Monitoring Alerts
      run: |
        echo "ğŸš¨ Configuring post-deployment monitoring..."
        
        # This would typically configure alerts in your monitoring system
        # For example, increased error rate monitoring, performance degradation alerts, etc.
        # Implementation depends on your monitoring stack (Prometheus, Datadog, etc.)
        
        echo "âœ… Production monitoring configured successfully!"

    - name: ğŸ‰ Deployment Success Notification
      uses: 8398a7/action-slack@v3
      with:
        status: success
        channel: '#imperial-deployments'
        message: |
          ğŸ‰ **Imperial Deployment Successful!**
          
          **Image:** `${{ needs.build-imperial-image.outputs.image-tag }}`
          **Commit:** `${{ github.sha }}`
          **Author:** `${{ github.actor }}`
          **Environment:** Production
          
          All citizens can now enjoy the latest imperial features! ğŸ‘‘
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}
      if: success()

    - name: ğŸš¨ Deployment Failure Notification
      uses: 8398a7/action-slack@v3
      with:
        status: failure
        channel: '#imperial-deployments'
        message: |
          ğŸš¨ **Imperial Deployment Failed!**
          
          **Image:** `${{ needs.build-imperial-image.outputs.image-tag }}`
          **Commit:** `${{ github.sha }}`
          **Author:** `${{ github.actor }}`
          **Environment:** Production
          
          The empire requires immediate attention! ğŸ”¥
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}
      if: failure()

  # ğŸ”„ Automated Rollback on Failure
  rollback-on-failure:
    name: ğŸ”„ Emergency Imperial Rollback
    needs: [deploy-canary, monitor-production]
    runs-on: ubuntu-latest
    if: failure() && github.ref == 'refs/heads/main'
    
    steps:
    - name: âš™ï¸ Configure Kubernetes for Rollback
      uses: azure/k8s-set-context@v3
      with:
        method: kubeconfig
        kubeconfig: ${{ secrets.KUBECONFIG_PRODUCTION }}

    - name: ğŸ”„ Execute Emergency Rollback
      run: |
        echo "ğŸš¨ EXECUTING EMERGENCY ROLLBACK..."
        
        # Rollback to previous stable version
        kubectl rollout undo deployment/imperial-palace -n fastapi-empire
        kubectl rollout status deployment/imperial-palace -n fastapi-empire --timeout=300s
        
        # Verify rollback success
        kubectl get pods -n fastapi-empire
        
        echo "âœ… Emergency rollback completed successfully!"

    - name: ğŸš¨ Emergency Notification
      uses: 8398a7/action-slack@v3
      with:
        status: custom
        custom_payload: |
          {
            "text": "ğŸš¨ EMERGENCY ROLLBACK EXECUTED",
            "attachments": [
              {
                "color": "danger",
                "fields": [
                  {
                    "title": "Failed Deployment",
                    "value": "${{ github.sha }}",
                    "short": true
                  },
                  {
                    "title": "Rollback Status",
                    "value": "Completed Successfully",
                    "short": true
                  }
                ]
              }
            ]
          }
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}
```

---

## ğŸ”§ Chapter 6: Advanced Troubleshooting & Performance

### ğŸ¯ The Imperial Diagnostics Center

When empires face challenges, wise rulers have systematic approaches to identify, diagnose, and resolve issues quickly. This chapter provides your royal troubleshooting playbook.

### ğŸ©º Common Imperial Ailments & Remedies

#### 1. ğŸ› Container Startup Failures

**ğŸ­ Symptoms**: Pods stuck in `CrashLoopBackOff` or `ImagePullBackOff`

```bash
# ğŸ” Imperial Diagnostic Commands
# Check pod status and events
kubectl describe pod <pod-name> -n fastapi-empire

# View container logs
kubectl logs <pod-name> -n fastapi-empire --previous

# Check resource constraints
kubectl top pods -n fastapi-empire

# Verify image availability
kubectl get events -n fastapi-empire --sort-by='.lastTimestamp'
```

**ğŸ› ï¸ Royal Solutions**:

```yaml
# Enhanced debugging deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: debug-imperial-palace
  namespace: fastapi-empire
spec:
  replicas: 1
  selector:
    matchLabels:
      app: debug-palace
  template:
    spec:
      containers:
      - name: debug-container
        image: your-registry.com/imperial-palace:latest
        # Debug startup issues with verbose logging
        command: ["/bin/bash"]
        args: ["-c", "set -x; env; ls -la /app; python -c 'import sys; print(sys.path)'; exec uvicorn main:app --host 0.0.0.0 --port 8000 --log-level debug"]
        
        # Increase resource limits for debugging
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        
        # Keep container alive for debugging
        lifecycle:
          preStop:
            exec:
              command: ["/bin/bash", "-c", "sleep 30"]
```

#### 2. ğŸš¦ Network Connectivity Issues

**ğŸ­ Symptoms**: Services cannot connect to each other, timeouts, DNS resolution failures

```bash
# ğŸ•µï¸ Imperial Network Investigation
# Test DNS resolution
kubectl exec -it <pod-name> -n fastapi-empire -- nslookup imperial-archives

# Test service connectivity
kubectl exec -it <pod-name> -n fastapi-empire -- curl -v http://imperial-cache:6379

# Check service endpoints
kubectl get endpoints -n fastapi-empire

# Verify network policies
kubectl describe networkpolicy -n fastapi-empire

# Test external connectivity
kubectl exec -it <pod-name> -n fastapi-empire -- curl -I https://google.com
```

**ğŸ› ï¸ Network Troubleshooting Pod**:

```yaml
# Network debugging Swiss Army knife
apiVersion: v1
kind: Pod
metadata:
  name: imperial-network-debugger
  namespace: fastapi-empire
spec:
  containers:
  - name: network-tools
    image: nicolaka/netshoot:latest
    command: ["sleep", "3600"]
    securityContext:
      capabilities:
        add: ["NET_ADMIN", "NET_RAW"]
    resources:
      limits:
        memory: "256Mi"
        cpu: "200m"
```

#### 3. ğŸ“Š Performance Bottlenecks

**ğŸ­ Symptoms**: Slow response times, high CPU/memory usage, timeouts

```bash
# ğŸ“ˆ Imperial Performance Analysis
# Monitor resource usage
kubectl top pods -n fastapi-empire --sort-by=cpu
kubectl top nodes

# Check application metrics
kubectl port-forward -n fastapi-empire service/imperial-palace-service 8080:8080
curl http://localhost:8080/metrics

# Analyze database performance
kubectl exec -it -n fastapi-empire statefulset/imperial-archives -- psql -U imperial_user -d imperial_db -c "
SELECT 
    query,
    calls,
    total_time,
    mean_time,
    rows
FROM pg_stat_statements 
ORDER BY total_time DESC 
LIMIT 10;"

# Check cache hit rates
kubectl exec -it -n fastapi-empire service/imperial-cache -- redis-cli info stats
```

**ğŸ”§ Performance Optimization Script**:

```python
# performance_monitor.py - Imperial Performance Guardian
import asyncio
import aiohttp
import time
import statistics
from datetime import datetime
import logging

class ImperialPerformanceMonitor:
    """
    Royal Performance Monitor - tracks and optimizes empire performance.
    
    Like having royal engineers constantly monitoring and optimizing
    all systems for maximum efficiency.
    """
    
    def __init__(self, base_url: str):
        self.base_url = base_url
        self.logger = logging.getLogger("imperial.performance")
        self.metrics = {
            "response_times": [],
            "error_rates": [],
            "throughput": []
        }
    
    async def measure_endpoint_performance(self, endpoint: str, duration: int = 60):
        """
        Measure endpoint performance over time.
        
        Like having royal timekeepers measure how quickly
        different court processes complete.
        """
        start_time = time.time()
        request_count = 0
        error_count = 0
        response_times = []
        
        async with aiohttp.ClientSession() as session:
            while time.time() - start_time < duration:
                try:
                    request_start = time.time()
                    async with session.get(f"{self.base_url}{endpoint}") as response:
                        request_end = time.time()
                        response_time = (request_end - request_start) * 1000  # ms
                        response_times.append(response_time)
                        
                        if response.status >= 400:
                            error_count += 1
                        
                        request_count += 1
                        
                except Exception as e:
                    self.logger.error(f"Request failed: {e}")
                    error_count += 1
                    request_count += 1
                
                await asyncio.sleep(1)  # 1 request per second
        
        # Calculate performance metrics
        if response_times:
            metrics = {
                "endpoint": endpoint,
                "duration": duration,
                "total_requests": request_count,
                "error_count": error_count,
                "error_rate": (error_count / request_count) * 100,
                "avg_response_time": statistics.mean(response_times),
                "p95_response_time": statistics.quantiles(response_times, n=20)[18],  # 95th percentile
                "p99_response_time": statistics.quantiles(response_times, n=100)[98],  # 99th percentile
                "throughput": request_count / duration,
                "timestamp": datetime.utcnow().isoformat()
            }
            
            self.logger.info(f"Performance metrics for {endpoint}: {metrics}")
            return metrics
        
        return None
    
    async def load_test_empire(self, concurrent_users: int = 10, duration: int = 300):
        """
        Simulate concurrent citizens accessing the empire.
        
        Like having multiple trade caravans arrive at the same time
        to test how well the empire handles busy periods.
        """
        self.logger.info(f"ğŸš€ Starting load test: {concurrent_users} concurrent users for {duration}s")
        
        endpoints = [
            "/health/live",
            "/health/ready", 
            "/users",
            "/health/detailed"
        ]
        
        tasks = []
        for i in range(concurrent_users):
            for endpoint in endpoints:
                task = asyncio.create_task(
                    self.measure_endpoint_performance(endpoint, duration // len(endpoints))
                )
                tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Aggregate results
        all_metrics = [r for r in results if isinstance(r, dict)]
        
        if all_metrics:
            summary = {
                "test_duration": duration,
                "concurrent_users": concurrent_users,
                "total_requests": sum(m["total_requests"] for m in all_metrics),
                "total_errors": sum(m["error_count"] for m in all_metrics),
                "avg_error_rate": statistics.mean([m["error_rate"] for m in all_metrics]),
                "avg_response_time": statistics.mean([m["avg_response_time"] for m in all_metrics]),
                "overall_throughput": sum(m["throughput"] for m in all_metrics),
                "timestamp": datetime.utcnow().isoformat()
            }
            
            self.logger.info(f"ğŸ“Š Load test summary: {summary}")
            return summary
        
        return None

# Usage example
async def run_imperial_performance_test():
    monitor = ImperialPerformanceMonitor("https://api.yourdomain.com")
    
    # Quick performance check
    await monitor.measure_endpoint_performance("/health/live", 30)
    
    # Full load test
    await monitor.load_test_empire(concurrent_users=50, duration=300)

if __name__ == "__main__":
    asyncio.run(run_imperial_performance_test())
```

### ğŸ¯ Imperial Performance Optimization Checklist

**ğŸ“‹ Application Layer Optimization**
- [ ] **ğŸ”„ Connection Pooling**: Use SQLAlchemy async pools with proper sizing
- [ ] **âš¡ Async Operations**: Convert all I/O operations to async/await
- [ ] **ğŸ“ Query Optimization**: Add database indexes for frequent queries
- [ ] **ğŸ—„ï¸ Caching Strategy**: Implement Redis caching for expensive operations
- [ ] **ğŸ“¦ Response Compression**: Enable gzip compression for large responses
- [ ] **ğŸ”€ Load Balancing**: Distribute traffic across multiple instances

**ğŸ“‹ Container Optimization**
- [ ] **ğŸ—ï¸ Multi-stage Builds**: Minimize image size with builder patterns
- [ ] **ğŸ‘¥ Non-root User**: Run containers with least-privilege users
- [ ] **ğŸ’¾ Resource Limits**: Set appropriate CPU and memory limits
- [ ] **ğŸ“ Read-only Filesystem**: Use read-only root filesystem for security
- [ ] **ğŸ” Health Checks**: Implement comprehensive liveness/readiness probes
- [ ] **âš¡ Fast Startup**: Optimize application startup time

**ğŸ“‹ Kubernetes Optimization**
- [ ] **ğŸ“ˆ Horizontal Pod Autoscaler**: Configure HPA for traffic spikes
- [ ] **ğŸ“Š Vertical Pod Autoscaler**: Optimize resource allocation
- [ ] **ğŸ¯ Pod Anti-affinity**: Distribute pods across nodes
- [ ] **ğŸ·ï¸ Node Affinity**: Schedule pods on optimal nodes
- [ ] **ğŸ’¿ Persistent Volumes**: Use appropriate storage classes
- [ ] **ğŸŒ Network Policies**: Implement security without performance impact

### ğŸ¯ Your Imperial Mastery Achievement

ğŸ‰ **Magnificent!** You've now mastered the complete container orchestration empire:

- âœ… **ğŸ—ï¸ Container Mastery**: From development to production-grade images
- âœ… **ğŸ™ï¸ Multi-Container Orchestration**: Complex applications with Docker Compose
- âœ… **â˜¸ï¸ Kubernetes Expertise**: Enterprise-scale container orchestration
- âœ… **ğŸ›¡ï¸ Security Implementation**: Zero-trust security and compliance
- âœ… **ğŸš€ CI/CD Automation**: Complete deployment pipeline automation
- âœ… **ğŸ”§ Troubleshooting Skills**: Systematic problem diagnosis and resolution
- âœ… **ğŸ“Š Performance Optimization**: Monitoring and optimization strategies

---

## ğŸ“ Your Journey from Village to Global Empire

### ğŸ° The Imperial Achievement Hierarchy

You've successfully completed the transformation from a simple village application to a globally distributed, enterprise-grade empire. Let's celebrate your achievements:

#### ğŸŒ± **Village Founder** (Docker Basics)
- Built your first standardized imperial building
- Mastered container fundamentals and best practices
- Implemented health monitoring and security

#### ğŸ™ï¸ **Metropolitan Planner** (Docker Compose)
- Orchestrated complex multi-service neighborhoods  
- Implemented production-ready networking and monitoring
- Mastered service discovery and load balancing

#### ğŸ›ï¸ **Imperial Administrator** (Kubernetes)
- Deployed applications at planetary scale
- Implemented auto-scaling and self-healing systems
- Mastered enterprise networking and ingress

#### ğŸ›¡ï¸ **Security Commander** (Advanced Security)
- Implemented zero-trust security architecture
- Mastered secret management and compliance
- Built comprehensive threat detection systems

#### ğŸš€ **DevOps Emperor** (CI/CD & Automation)
- Created fully automated deployment pipelines
- Implemented canary deployments and rollback strategies
- Mastered performance monitoring and optimization

### ğŸŒ Your Next Imperial Conquests

**ğŸš€ Advanced Territories to Explore:**

1. **ğŸŒ Multi-Region Deployments**
   - Master global application distribution
   - Implement data replication strategies
   - Handle cross-region networking and latency

2. **ğŸ¤– Machine Learning Integration**
   - Deploy ML models with Kubernetes
   - Implement model versioning and A/B testing
   - Master GPU workload orchestration

3. **ğŸ“Š Advanced Observability**
   - Implement distributed tracing
   - Master chaos engineering practices
   - Build predictive monitoring systems

4. **ğŸ”® Emerging Technologies**
   - Service mesh with Istio
   - Serverless with Knative
   - Edge computing deployments

### ğŸ¯ The Imperial Wisdom

Remember these fundamental principles as you continue building your digital empire:

> **"Excellence is not a destination, but a continuous journey of improvement, learning, and adaptation to serve your citizens better."**

1. **ğŸ—ï¸ Start Simple, Scale Smart**: Begin with solid foundations, then add complexity gradually
2. **ğŸ›¡ï¸ Security First**: Build security into every layer from the beginning
3. **ğŸ“Š Measure Everything**: You can't optimize what you don't measure
4. **ğŸ”„ Automate Ruthlessly**: If you do it twice, automate it the third time
5. **ğŸ¯ Focus on Citizens**: Every technical decision should improve user experience
6. **ğŸ“š Never Stop Learning**: Technology evolves; your skills must evolve too

Your FastAPI empire now stands as a testament to modern software engineering excellence. You've mastered the art of transforming simple code into scalable, secure, and maintainable systems that can serve millions of users worldwide.

**ğŸŠ Congratulations, Your Imperial Majesty! Your empire awaits your continued leadership and innovation!** ğŸ‘‘

---

*ğŸ’« "The best empires are built not by those who seek to rule, but by those who seek to serve their citizens with excellence, security, and reliability."*

