# Advanced Features

Explore advanced capabilities of the Anthropic Python SDK for sophisticated applications.

## Topics Covered

Advanced features and techniques for power users:

- **[Streaming Responses →](/docs/advanced-features/streaming)** - Real-time response generation
- **[Async Programming →](/docs/advanced-features/async)** - Non-blocking API calls
- **[Custom Tools →](/docs/advanced-features/tools)** - Extend Claude's capabilities
- **[Prompt Engineering →](/docs/advanced-features/prompting)** - Advanced prompting techniques
- **[Performance Optimization →](/docs/advanced-features/performance)** - Speed and efficiency tips

## Quick Overview

### Streaming Responses

Get responses as they're generated:

```python
stream = client.messages.create(
    model="claude-3-opus-20240229",
    max_tokens=1024,
    stream=True,
    messages=[{"role": "user", "content": "Write a story"}]
)

for chunk in stream:
    if chunk.type == "content_block_delta":
        print(chunk.delta.text, end="", flush=True)
```

### Async API Calls

Make non-blocking requests:

```python
import asyncio
from anthropic import AsyncAnthropic

async def main():
    client = AsyncAnthropic()
    
    response = await client.messages.create(
        model="claude-3-opus-20240229",
        max_tokens=1024,
        messages=[{"role": "user", "content": "Hello"}]
    )
    
    print(response.content[0].text)

asyncio.run(main())
```

### Parallel Processing

Handle multiple requests efficiently:

```python
import asyncio
from anthropic import AsyncAnthropic

async def process_multiple_prompts(prompts):
    client = AsyncAnthropic()
    
    async def process_single(prompt):
        response = await client.messages.create(
            model="claude-3-opus-20240229",
            max_tokens=512,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.content[0].text
    
    # Process all prompts concurrently
    tasks = [process_single(prompt) for prompt in prompts]
    results = await asyncio.gather(*tasks)
    
    return results

# Usage
prompts = [
    "Explain Python lists",
    "What are Python dictionaries?",
    "How do Python functions work?"
]

results = asyncio.run(process_multiple_prompts(prompts))
for i, result in enumerate(results):
    print(f"Result {i+1}: {result}")
```

## Advanced Conversation Management

### Conversation Branching

Create multiple conversation paths:

```python
class BranchingConversation:
    def __init__(self, client):
        self.client = client
        self.base_messages = []
        self.branches = {}
    
    def set_base(self, messages):
        """Set the base conversation"""
        self.base_messages = messages.copy()
    
    def create_branch(self, branch_name, additional_messages):
        """Create a new conversation branch"""
        branch_messages = self.base_messages + additional_messages
        self.branches[branch_name] = branch_messages
        return branch_messages
    
    def continue_branch(self, branch_name, message):
        """Continue a specific branch"""
        if branch_name not in self.branches:
            raise ValueError(f"Branch {branch_name} does not exist")
        
        branch_messages = self.branches[branch_name]
        branch_messages.append({"role": "user", "content": message})
        
        response = self.client.messages.create(
            model="claude-3-opus-20240229",
            max_tokens=1024,
            messages=branch_messages
        )
        
        assistant_message = response.content[0].text
        branch_messages.append({"role": "assistant", "content": assistant_message})
        
        return assistant_message

# Usage
branching = BranchingConversation(client)
branching.set_base([
    {"role": "user", "content": "I want to learn programming"}
])

# Create different learning paths
python_branch = branching.create_branch("python", [
    {"role": "assistant", "content": "Great! Python is an excellent choice for beginners."}
])

javascript_branch = branching.create_branch("javascript", [
    {"role": "assistant", "content": "Excellent! JavaScript is perfect for web development."}
])

# Continue each branch independently
python_response = branching.continue_branch("python", "What should I learn first?")
js_response = branching.continue_branch("javascript", "How do I get started?")
```

### Memory Management

Implement sophisticated memory systems:

```python
import json
from typing import Dict, List, Any

class ConversationMemory:
    def __init__(self, max_short_term=10, max_long_term=50):
        self.short_term = []  # Recent messages
        self.long_term = []   # Summarized important information
        self.facts = {}       # Extracted facts about the user
        self.max_short_term = max_short_term
        self.max_long_term = max_long_term
    
    def add_message(self, role: str, content: str):
        """Add message to short-term memory"""
        self.short_term.append({"role": role, "content": content})
        
        # Manage memory size
        if len(self.short_term) > self.max_short_term:
            self._compress_memory()
    
    def _compress_memory(self):
        """Compress old messages into long-term memory"""
        # Take the oldest messages for summarization
        to_summarize = self.short_term[:5]
        self.short_term = self.short_term[5:]
        
        # Summarize (this would call Claude)
        summary = self._summarize_messages(to_summarize)
        self.long_term.append(summary)
        
        # Limit long-term memory size
        if len(self.long_term) > self.max_long_term:
            self.long_term = self.long_term[-self.max_long_term:]
    
    def _summarize_messages(self, messages):
        """Summarize a list of messages (placeholder)"""
        # In real implementation, this would call Claude to summarize
        return f"Summary of {len(messages)} messages: [content summarized]"
    
    def extract_facts(self, content: str):
        """Extract facts about the user from their messages"""
        # In practice, you'd use Claude to extract structured information
        if "my name is" in content.lower():
            import re
            name_match = re.search(r"my name is (\w+)", content.lower())
            if name_match:
                self.facts["name"] = name_match.group(1)
    
    def get_context_messages(self):
        """Get messages for API call including context"""
        context_messages = []
        
        # Add long-term memory as context
        if self.long_term:
            context = "Previous conversation context: " + " ".join(self.long_term[-3:])
            context_messages.append({"role": "system", "content": context})
        
        # Add facts about user
        if self.facts:
            facts_context = f"User information: {json.dumps(self.facts)}"
            context_messages.append({"role": "system", "content": facts_context})
        
        # Add recent messages
        context_messages.extend(self.short_term)
        
        return context_messages
```

## Advanced Error Handling

### Circuit Breaker Pattern

Prevent cascading failures:

```python
import time
from enum import Enum

class CircuitState(Enum):
    CLOSED = "closed"      # Normal operation
    OPEN = "open"         # Failing, reject requests
    HALF_OPEN = "half_open"  # Testing if service recovered

class CircuitBreaker:
    def __init__(self, failure_threshold=5, timeout=60):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = CircuitState.CLOSED
    
    def call(self, func, *args, **kwargs):
        """Execute function with circuit breaker protection"""
        if self.state == CircuitState.OPEN:
            if time.time() - self.last_failure_time > self.timeout:
                self.state = CircuitState.HALF_OPEN
            else:
                raise Exception("Circuit breaker is OPEN")
        
        try:
            result = func(*args, **kwargs)
            self._on_success()
            return result
        except Exception as e:
            self._on_failure()
            raise
    
    def _on_success(self):
        """Handle successful call"""
        self.failure_count = 0
        self.state = CircuitState.CLOSED
    
    def _on_failure(self):
        """Handle failed call"""
        self.failure_count += 1
        self.last_failure_time = time.time()
        
        if self.failure_count >= self.failure_threshold:
            self.state = CircuitState.OPEN

# Usage
breaker = CircuitBreaker(failure_threshold=3, timeout=30)

def protected_api_call(client, **kwargs):
    return breaker.call(client.messages.create, **kwargs)
```

## Performance Optimization

### Request Batching

Optimize multiple related requests:

```python
import asyncio
from anthropic import AsyncAnthropic

class RequestBatcher:
    def __init__(self, batch_size=5, delay=0.1):
        self.batch_size = batch_size
        self.delay = delay
        self.pending_requests = []
        self.client = AsyncAnthropic()
    
    async def add_request(self, **kwargs):
        """Add request to batch"""
        future = asyncio.Future()
        self.pending_requests.append((kwargs, future))
        
        # Process batch if full
        if len(self.pending_requests) >= self.batch_size:
            await self._process_batch()
        
        return await future
    
    async def _process_batch(self):
        """Process all pending requests"""
        if not self.pending_requests:
            return
        
        batch = self.pending_requests[:self.batch_size]
        self.pending_requests = self.pending_requests[self.batch_size:]
        
        # Execute all requests concurrently
        tasks = []
        for kwargs, future in batch:
            task = self._execute_request(kwargs, future)
            tasks.append(task)
        
        await asyncio.gather(*tasks, return_exceptions=True)
    
    async def _execute_request(self, kwargs, future):
        """Execute a single request"""
        try:
            response = await self.client.messages.create(**kwargs)
            future.set_result(response)
        except Exception as e:
            future.set_exception(e)
    
    async def flush(self):
        """Process remaining requests"""
        while self.pending_requests:
            await self._process_batch()

# Usage
batcher = RequestBatcher()

async def main():
    # Add multiple requests
    tasks = []
    for i in range(10):
        task = batcher.add_request(
            model="claude-3-opus-20240229",
            max_tokens=100,
            messages=[{"role": "user", "content": f"Count to {i+1}"}]
        )
        tasks.append(task)
    
    # Wait for all to complete
    results = await asyncio.gather(*tasks)
    
    for i, response in enumerate(results):
        print(f"Response {i+1}: {response.content[0].text}")

asyncio.run(main())
```

[← Previous: Error Handling](/docs/basic-usage/error-handling) | [Next: Streaming →](/docs/advanced-features/streaming)