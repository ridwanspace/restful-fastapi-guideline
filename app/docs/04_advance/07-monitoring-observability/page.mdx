# ðŸ“Š Monitoring & Observability

## 14. Monitoring & Observability

### Request Logging and Metrics

Comprehensive logging and monitoring setup:

```python
from fastapi import FastAPI, Request, Response
import logging
import time
import psutil
import json
from datetime import datetime
from typing import Dict, Any
from prometheus_client import Counter, Histogram, Gauge, generate_latest
import asyncio

# Configure structured logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# Create logger
logger = logging.getLogger(__name__)

# Prometheus metrics
REQUEST_COUNT = Counter(
    'api_requests_total',
    'Total API requests',
    ['method', 'endpoint', 'status_code']
)

REQUEST_DURATION = Histogram(
    'api_request_duration_seconds',
    'API request duration',
    ['method', 'endpoint']
)

ACTIVE_CONNECTIONS = Gauge(
    'api_active_connections',
    'Active API connections'
)

ERROR_COUNT = Counter(
    'api_errors_total',
    'Total API errors',
    ['error_type', 'endpoint']
)

CACHE_HITS = Counter(
    'api_cache_hits_total',
    'Cache hits',
    ['cache_type']
)

CACHE_MISSES = Counter(
    'api_cache_misses_total',
    'Cache misses',
    ['cache_type']
)

# System metrics
SYSTEM_CPU_USAGE = Gauge('system_cpu_usage_percent', 'System CPU usage')
SYSTEM_MEMORY_USAGE = Gauge('system_memory_usage_percent', 'System memory usage')

class MetricsCollector:
    """Collect and expose application metrics"""
    
    def __init__(self):
        self.start_time = time.time()
        self.request_stats = {
            'total_requests': 0,
            'error_count': 0,
            'avg_response_time': 0.0
        }
    
    def record_request(self, method: str, endpoint: str, status_code: int, duration: float):
        """Record request metrics"""
        REQUEST_COUNT.labels(method=method, endpoint=endpoint, status_code=status_code).inc()
        REQUEST_DURATION.labels(method=method, endpoint=endpoint).observe(duration)
        
        # Update internal stats
        self.request_stats['total_requests'] += 1
        if status_code >= 400:
            self.request_stats['error_count'] += 1
            ERROR_COUNT.labels(error_type=f"{status_code}", endpoint=endpoint).inc()
        
        # Update average response time
        current_avg = self.request_stats['avg_response_time']
        total_requests = self.request_stats['total_requests']
        self.request_stats['avg_response_time'] = (
            (current_avg * (total_requests - 1) + duration) / total_requests
        )
    
    def get_uptime(self) -> float:
        return time.time() - self.start_time
    
    def collect_system_metrics(self):
        """Collect system-level metrics"""
        SYSTEM_CPU_USAGE.set(psutil.cpu_percent())
        SYSTEM_MEMORY_USAGE.set(psutil.virtual_memory().percent)

metrics_collector = MetricsCollector()

# Logging middleware with structured logs
@app.middleware("http")
async def logging_middleware(request: Request, call_next):
    """Comprehensive request/response logging"""
    
    start_time = time.time()
    request_id = request.headers.get("x-request-id", str(uuid.uuid4()))
    
    # Extract request information
    client_ip = request.client.host if request.client else "unknown"
    user_agent = request.headers.get("user-agent", "unknown")
    
    # Log request start
    request_log_data = {
        "event": "request_start",
        "request_id": request_id,
        "method": request.method,
        "url": str(request.url),
        "path": request.url.path,
        "query_params": dict(request.query_params),
        "client_ip": client_ip,
        "user_agent": user_agent,
        "timestamp": datetime.utcnow().isoformat() + "Z"
    }
    
    logger.info("Request started", extra=request_log_data)
    
    # Process request
    try:
        ACTIVE_CONNECTIONS.inc()
        response = await call_next(request)
        
        # Calculate processing time
        process_time = time.time() - start_time
        
        # Log response
        response_log_data = {
            "event": "request_complete",
            "request_id": request_id,
            "status_code": response.status_code,
            "process_time": round(process_time, 4),
            "response_size": response.headers.get("content-length"),
            "timestamp": datetime.utcnow().isoformat() + "Z"
        }
        
        # Add custom response headers
        response.headers["X-Request-ID"] = request_id
        response.headers["X-Process-Time"] = str(round(process_time, 4))
        
        # Record metrics
        endpoint = request.url.path
        metrics_collector.record_request(
            request.method, endpoint, response.status_code, process_time
        )
        
        # Log based on status code
        if response.status_code >= 400:
            logger.warning("Request failed", extra=response_log_data)
        else:
            logger.info("Request completed", extra=response_log_data)
        
        return response
        
    except Exception as e:
        process_time = time.time() - start_time
        
        # Log error
        error_log_data = {
            "event": "request_error",
            "request_id": request_id,
            "error_type": type(e).__name__,
            "error_message": str(e),
            "process_time": round(process_time, 4),
            "timestamp": datetime.utcnow().isoformat() + "Z"
        }
        
        logger.error("Request error", extra=error_log_data, exc_info=True)
        
        # Record error metrics
        ERROR_COUNT.labels(error_type=type(e).__name__, endpoint=request.url.path).inc()
        
        raise
    
    finally:
        ACTIVE_CONNECTIONS.dec()

# Health check endpoints
@app.get("/health")
async def health_check():
    """Basic health check"""
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "uptime": metrics_collector.get_uptime(),
        "version": "1.0.0"
    }

@app.get("/health/detailed")
async def detailed_health_check():
    """Detailed health check with dependencies"""
    
    checks = {
        "api": {"status": "healthy", "response_time": 0.001},
        "database": {"status": "unknown", "response_time": None},
        "cache": {"status": "unknown", "response_time": None},
        "external_service": {"status": "unknown", "response_time": None}
    }
    
    # Check database connectivity
    try:
        db_start = time.time()
        # Simulate database ping
        await asyncio.sleep(0.01)  # Mock DB query
        checks["database"] = {
            "status": "healthy",
            "response_time": round(time.time() - db_start, 4)
        }
    except Exception as e:
        checks["database"] = {
            "status": "unhealthy",
            "error": str(e),
            "response_time": None
        }
    
    # Check cache connectivity
    try:
        cache_start = time.time()
        redis_client.get("health_check")
        checks["cache"] = {
            "status": "healthy",
            "response_time": round(time.time() - cache_start, 4)
        }
    except Exception as e:
        checks["cache"] = {
            "status": "unhealthy",
            "error": str(e),
            "response_time": None
        }
    
    # Overall status
    overall_status = "healthy"
    if any(check["status"] == "unhealthy" for check in checks.values()):
        overall_status = "unhealthy"
    elif any(check["status"] == "unknown" for check in checks.values()):
        overall_status = "degraded"
    
    return {
        "status": overall_status,
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "uptime": metrics_collector.get_uptime(),
        "checks": checks,
        "system": {
            "cpu_usage": psutil.cpu_percent(),
            "memory_usage": psutil.virtual_memory().percent,
            "disk_usage": psutil.disk_usage('/').percent
        }
    }

@app.get("/metrics")
async def get_metrics():
    """Prometheus metrics endpoint"""
    
    # Collect current system metrics
    metrics_collector.collect_system_metrics()
    
    # Generate Prometheus format
    metrics_output = generate_latest()
    
    return Response(
        content=metrics_output,
        media_type="text/plain; version=0.0.4; charset=utf-8"
    )

@app.get("/metrics/application")
async def get_application_metrics():
    """Custom application metrics in JSON format"""
    
    return {
        "uptime_seconds": metrics_collector.get_uptime(),
        "request_stats": metrics_collector.request_stats,
        "system_stats": {
            "cpu_percent": psutil.cpu_percent(),
            "memory_percent": psutil.virtual_memory().percent,
            "disk_percent": psutil.disk_usage('/').percent,
            "active_connections": len(asyncio.all_tasks())
        },
        "api_stats": {
            "endpoints_count": len(app.routes),
            "cache_hit_rate": calculate_cache_hit_rate(),
            "error_rate": calculate_error_rate()
        },
        "timestamp": datetime.utcnow().isoformat() + "Z"
    }

def calculate_cache_hit_rate() -> float:
    """Calculate cache hit rate"""
    # Mock calculation - in production, use actual cache metrics
    return 0.85  # 85% hit rate

def calculate_error_rate() -> float:
    """Calculate error rate"""
    stats = metrics_collector.request_stats
    if stats['total_requests'] == 0:
        return 0.0
    return stats['error_count'] / stats['total_requests']

# Request tracing for distributed systems
class RequestTracer:
    """Simple request tracing for distributed systems"""
    
    def __init__(self):
        self.traces = {}
    
    def start_trace(self, request_id: str, operation: str) -> str:
        """Start a new trace span"""
        span_id = str(uuid.uuid4())
        
        if request_id not in self.traces:
            self.traces[request_id] = []
        
        self.traces[request_id].append({
            "span_id": span_id,
            "operation": operation,
            "start_time": time.time(),
            "end_time": None,
            "duration": None,
            "tags": {}
        })
        
        return span_id
    
    def finish_trace(self, request_id: str, span_id: str, tags: Dict[str, Any] = None):
        """Finish a trace span"""
        if request_id in self.traces:
            for span in self.traces[request_id]:
                if span["span_id"] == span_id:
                    span["end_time"] = time.time()
                    span["duration"] = span["end_time"] - span["start_time"]
                    if tags:
                        span["tags"].update(tags)
                    break
    
    def get_trace(self, request_id: str) -> Dict[str, Any]:
        """Get complete trace for request"""
        return {
            "request_id": request_id,
            "spans": self.traces.get(request_id, []),
            "total_duration": sum(
                span.get("duration", 0) 
                for span in self.traces.get(request_id, [])
                if span.get("duration")
            )
        }

tracer = RequestTracer()

@app.get("/trace/{request_id}")
async def get_request_trace(request_id: str):
    """Get distributed trace for a request"""
    trace_data = tracer.get_trace(request_id)
    
    if not trace_data["spans"]:
        raise HTTPException(status_code=404, detail="Trace not found")
    
    return trace_data

# Example traced endpoint
@app.get("/users/{user_id}/traced")
async def get_user_with_tracing(
    user_id: int,
    request: Request
):
    """Example endpoint with distributed tracing"""
    
    request_id = request.headers.get("x-request-id", str(uuid.uuid4()))
    
    # Start main operation trace
    main_span = tracer.start_trace(request_id, "get_user")
    
    try:
        # Trace database operation
        db_span = tracer.start_trace(request_id, "database_query")
        await asyncio.sleep(0.05)  # Simulate DB query
        tracer.finish_trace(request_id, db_span, {"table": "users", "query_type": "select"})
        
        # Trace cache operation
        cache_span = tracer.start_trace(request_id, "cache_lookup")
        await asyncio.sleep(0.01)  # Simulate cache lookup
        tracer.finish_trace(request_id, cache_span, {"cache_type": "redis", "hit": False})
        
        # Simulate user data
        user_data = {
            "id": user_id,
            "username": f"user_{user_id}",
            "email": f"user_{user_id}@example.com"
        }
        
        tracer.finish_trace(request_id, main_span, {"user_found": True})
        
        return {
            "data": user_data,
            "trace_id": request_id,
            "trace_url": f"/trace/{request_id}"
        }
        
    except Exception as e:
        tracer.finish_trace(request_id, main_span, {"error": str(e)})
        raise
```
---