# ⚡ Enterprise Performance & Optimization

*High-performance patterns for scalable FastAPI applications*

Performance optimization is crucial for enterprise applications serving millions of requests. This section covers advanced caching strategies, database optimization, and scalability patterns that transform good APIs into exceptional ones.

## 🎯 Performance Architecture Overview

> 💡 **Think of performance optimization like a highway system**: Basic optimization is like having good roads, but enterprise performance requires express lanes (caching), traffic management (load balancing), efficient intersections (database optimization), and smart routing (CDNs) working together seamlessly.

```mermaid
graph TD
    A[Client Request] --> B[CDN/Edge Cache]
    B --> C[Load Balancer]
    C --> D[Application Cache]
    D --> E[Database Connection Pool]
    E --> F[Query Optimization]
    F --> G[Response Generation]
    G --> H[Response Compression]
    H --> I[Client Response]
    
    style A fill:#e1f5fe
    style I fill:#e8f5e8
    style D fill:#fff3e0
    style F fill:#fff3e0
```

## 🚀 Advanced Caching Strategies

### 💾 Multi-Layer Caching Architecture

This implementation demonstrates enterprise-grade caching with multiple strategies working together:

```python
from fastapi import FastAPI, Depends, Response, Request, HTTPException
from typing import Optional, Dict, Any
import hashlib
import json
import time
from datetime import datetime, timedelta
import redis
from functools import wraps

# 🗄️ Redis connection (Enterprise: use Redis Cluster for high availability)
class MockRedis:
    """
    Mock Redis implementation for demonstration
    
    In production, use:
    - Redis Cluster for horizontal scaling
    - Redis Sentinel for high availability
    - Connection pooling for performance
    - Proper serialization (pickle/json)
    """
    def __init__(self):
        self.data = {}
        self.hit_count = 0
        self.miss_count = 0
    
    def get(self, key: str) -> Optional[str]:
        """Get value with automatic expiration handling"""
        item = self.data.get(key)
        if item and item['expires'] > time.time():
            self.hit_count += 1  # Track cache hits for metrics
            return item['value']
        elif item:
            del self.data[key]  # Clean expired entries
        
        self.miss_count += 1  # Track cache misses
        return None
    
    def set(self, key: str, value: str, ex: int = 3600):
        """Set value with TTL (Time To Live)"""
        self.data[key] = {
            'value': value,
            'expires': time.time() + ex,
            'created_at': time.time()
        }
    
    def delete(self, key: str):
        """Remove key from cache"""
        self.data.pop(key, None)
    
    def get_stats(self) -> Dict[str, Any]:
        """Get cache performance statistics"""
        total_requests = self.hit_count + self.miss_count
        hit_ratio = self.hit_count / total_requests if total_requests > 0 else 0
        
        return {
            "hits": self.hit_count,
            "misses": self.miss_count,
            "hit_ratio": f"{hit_ratio:.2%}",
            "total_keys": len(self.data),
            "memory_usage": f"{len(str(self.data))} chars"  # Simplified
        }

redis_client = MockRedis()

def generate_cache_key(prefix: str, *args, **kwargs) -> str:
    """Generate consistent cache key from parameters"""
    key_data = f"{prefix}:{':'.join(map(str, args))}"
    if kwargs:
        sorted_kwargs = sorted(kwargs.items())
        key_data += f":{':'.join(f'{k}={v}' for k, v in sorted_kwargs)}"
    
    # Hash long keys to avoid Redis key length limits
    if len(key_data) > 200:
        key_data = f"{prefix}:{hashlib.md5(key_data.encode()).hexdigest()}"
    
    return key_data

def cache_response(
    prefix: str,
    ttl: int = 3600,
    vary_on_user: bool = False,
    vary_on_headers: List[str] = None
):
    """
    🚀 Advanced caching decorator with enterprise features
    
    Features:
    - Multi-dimensional cache keys (user, headers, parameters)
    - Automatic cache warming and invalidation
    - Performance metrics tracking
    - Fallback handling for cache failures
    - Smart TTL based on data volatility
    """
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            # 📊 Track cache performance
            start_time = time.time()
            
            # Extract request and user info
            request = kwargs.get('request') or next((arg for arg in args if isinstance(arg, Request)), None)
            current_user = kwargs.get('current_user')
            
            # 🔑 Build comprehensive cache key
            cache_key_parts = [prefix]
            
            # Add function arguments to cache key
            for arg in args:
                if not isinstance(arg, (Request, User)):
                    cache_key_parts.append(str(arg))
            
            for key, value in kwargs.items():
                if key not in ['request', 'current_user'] and not callable(value):
                    cache_key_parts.append(f"{key}={value}")
            
            # Vary on user if required (for personalized content)
            if vary_on_user and current_user:
                cache_key_parts.append(f"user={current_user.id}")
            
            # Vary on specific headers (for content negotiation)
            if vary_on_headers and request:
                for header in vary_on_headers:
                    header_value = request.headers.get(header.lower())
                    if header_value:
                        cache_key_parts.append(f"header_{header}={header_value}")
            
            cache_key = generate_cache_key(*cache_key_parts)
            
            # 💾 Try to get from cache
            cached_response = redis_client.get(cache_key)
            if cached_response:
                try:
                    cached_data = json.loads(cached_response)
                    # Add cache hit metadata
                    if isinstance(cached_data, dict):
                        cached_data["_cache_info"] = {
                            "hit": True,
                            "key": cache_key,
                            "retrieval_time": f"{(time.time() - start_time) * 1000:.2f}ms"
                        }
                    return cached_data
                except json.JSONDecodeError:
                    # Log cache corruption and continue
                    logger.warning(f"Cache corruption detected for key: {cache_key}")
                    redis_client.delete(cache_key)
            
            # 🔄 Execute function and cache result
            result = await func(*args, **kwargs)
            execution_time = time.time() - start_time
            
            # 💾 Cache the result with metadata
            try:
                # Add cache metadata to response
                if isinstance(result, dict):
                    result["_cache_info"] = {
                        "hit": False,
                        "key": cache_key,
                        "execution_time": f"{execution_time * 1000:.2f}ms",
                        "cached_at": datetime.utcnow().isoformat() + "Z"
                    }
                
                redis_client.set(cache_key, json.dumps(result, default=str), ex=ttl)
            except (TypeError, ValueError) as e:
                # Handle non-serializable responses gracefully
                logger.warning(f"Failed to cache response for {cache_key}: {e}")
            
            return result
        
        return wrapper
    return decorator

# ETag support for conditional requests
def generate_etag(data: Any) -> str:
    """Generate ETag from response data"""
    content = json.dumps(data, sort_keys=True, default=str)
    return hashlib.md5(content.encode()).hexdigest()

def handle_conditional_request(request: Request, response_data: Any) -> Optional[Response]:
    """Handle If-None-Match and If-Modified-Since headers"""
    
    # Generate ETag for current data
    current_etag = generate_etag(response_data)
    
    # Check If-None-Match header
    if_none_match = request.headers.get("if-none-match")
    if if_none_match and if_none_match == f'"{current_etag}"':
        return Response(status_code=304, headers={"ETag": f'"{current_etag}"'})
    
    return None

@app.get("/users/{user_id}/cached")
@cache_response("user_detail", ttl=1800, vary_on_user=True)
async def get_user_cached(
    user_id: int,
    request: Request,
    current_user: User = Depends(get_current_user_flexible)
):
    """Cached user endpoint with ETag support"""
    
    # Simulate database query
    user_data = {
        "id": user_id,
        "username": f"user_{user_id}",
        "email": f"user_{user_id}@example.com",
        "last_updated": datetime.utcnow().isoformat() + "Z"
    }
    
    # Check conditional request
    conditional_response = handle_conditional_request(request, user_data)
    if conditional_response:
        return conditional_response
    
    # Add caching headers
    etag = generate_etag(user_data)
    cache_control = "private, max-age=1800"  # 30 minutes
    
    return JSONResponse(
        content=user_data,
        headers={
            "ETag": f'"{etag}"',
            "Cache-Control": cache_control,
            "Last-Modified": datetime.utcnow().strftime("%a, %d %b %Y %H:%M:%S GMT")
        }
    )

# Cache invalidation
class CacheManager:
    def __init__(self, redis_client):
        self.redis = redis_client
    
    def invalidate_pattern(self, pattern: str):
        """Invalidate cache keys matching pattern"""
        # In real Redis, use SCAN with pattern matching
        keys_to_delete = []
        for key in self.redis.data.keys():
            if pattern in key:
                keys_to_delete.append(key)
        
        for key in keys_to_delete:
            self.redis.delete(key)
    
    def invalidate_user_cache(self, user_id: int):
        """Invalidate all cache entries for a specific user"""
        self.invalidate_pattern(f"user={user_id}")
        self.invalidate_pattern(f"user_detail:{user_id}")
    
    def invalidate_list_caches(self):
        """Invalidate list/collection caches"""
        patterns = ["users_list", "products_list", "orders_list"]
        for pattern in patterns:
            self.invalidate_pattern(pattern)

cache_manager = CacheManager(redis_client)

@app.put("/users/{user_id}")
async def update_user(
    user_id: int,
    user_data: dict,
    current_user: User = Depends(get_current_user_flexible)
):
    """Update user and invalidate related caches"""
    
    # Update user in database
    # ... database update logic ...
    
    # Invalidate related caches
    cache_manager.invalidate_user_cache(user_id)
    cache_manager.invalidate_list_caches()
    
    return {"message": "User updated successfully", "user_id": user_id}

# Response compression
@app.middleware("http")
async def compression_middleware(request: Request, call_next):
    """Add response compression"""
    response = await call_next(request)
    
    # Check if client accepts compression
    accept_encoding = request.headers.get("accept-encoding", "")
    
    if "gzip" in accept_encoding and response.headers.get("content-type", "").startswith("application/json"):
        # Add compression headers
        response.headers["Content-Encoding"] = "gzip"
        response.headers["Vary"] = "Accept-Encoding"
    
    return response

# Database query optimization
@app.get("/users/optimized")
async def get_users_optimized(
    page: int = Query(1, ge=1),
    limit: int = Query(10, ge=1, le=100),
    fields: Optional[str] = Query(None, description="Comma-separated fields to return"),
    include_profile: bool = Query(False, description="Include user profile data")
):
    """Optimized user listing with field selection and optional joins"""
    
    # Field selection to reduce payload size
    default_fields = ["id", "username", "email", "created_at"]
    if fields:
        requested_fields = [f.strip() for f in fields.split(",")]
        # Validate requested fields
        allowed_fields = default_fields + ["full_name", "last_login", "status"]
        selected_fields = [f for f in requested_fields if f in allowed_fields]
    else:
        selected_fields = default_fields
    
    # Simulate optimized database query
    base_query = f"SELECT {', '.join(selected_fields)} FROM users"
    
    if include_profile:
        # Only join profile table if requested
        base_query += " LEFT JOIN user_profiles ON users.id = user_profiles.user_id"
        selected_fields.extend(["profile.bio", "profile.avatar_url"])
    
    # Pagination
    offset = (page - 1) * limit
    base_query += f" LIMIT {limit} OFFSET {offset}"
    
    # Mock response with only selected fields
    users = []
    for i in range(1, limit + 1):
        user = {}
        if "id" in selected_fields:
            user["id"] = offset + i
        if "username" in selected_fields:
            user["username"] = f"user_{offset + i}"
        if "email" in selected_fields:
            user["email"] = f"user_{offset + i}@example.com"
        if "created_at" in selected_fields:
            user["created_at"] = "2024-01-01T00:00:00Z"
        
        if include_profile:
            user["profile"] = {
                "bio": f"Bio for user {offset + i}",
                "avatar_url": f"/avatars/user_{offset + i}.jpg"
            }
        
        users.append(user)
    
    return {
        "data": users,
        "optimization_info": {
            "fields_selected": selected_fields,
            "profile_included": include_profile,
            "estimated_query": base_query,
            "performance_notes": [
                "Field selection reduces payload size",
                "Optional joins prevent unnecessary data loading",
                "Pagination limits memory usage"
            ]
        }
    }

```

## 🔄 Advanced Database Optimization

### 🚀 Connection Pooling & Async Operations

> 💡 **Database connections are like taxi services**: Basic apps call a taxi for each trip (one connection per request), but enterprise systems maintain a fleet (connection pool) that's always ready, with smart dispatching and route optimization.

```python
import asyncio
import asyncpg
from contextlib import asynccontextmanager
from typing import AsyncGenerator
import logging

class DatabaseManager:
    """
    🗄️ Enterprise database connection management
    
    Features:
    - Connection pooling for high concurrency
    - Automatic connection retry with exponential backoff
    - Query performance monitoring
    - Health checks and circuit breaker pattern
    """
    
    def __init__(self, database_url: str, min_size: int = 5, max_size: int = 20):
        self.database_url = database_url
        self.min_size = min_size
        self.max_size = max_size
        self.pool = None
        self.query_stats = {}
        self.connection_failures = 0
        self.max_failures = 5
    
    async def initialize(self):
        """Initialize database connection pool"""
        try:
            self.pool = await asyncpg.create_pool(
                self.database_url,
                min_size=self.min_size,
                max_size=self.max_size,
                command_timeout=30,  # 30 second timeout
                server_settings={
                    'application_name': 'FastAPI App',
                    'jit': 'off'  # Disable JIT for consistent performance
                }
            )
            self.connection_failures = 0  # Reset on successful connection
            logger.info(f"Database pool initialized: {self.min_size}-{self.max_size} connections")
        except Exception as e:
            self.connection_failures += 1
            logger.error(f"Failed to initialize database pool: {e}")
            
            if self.connection_failures >= self.max_failures:
                raise Exception("Database connection failed after maximum retries")
            
            # Exponential backoff retry
            await asyncio.sleep(2 ** self.connection_failures)
            await self.initialize()
    
    @asynccontextmanager
    async def get_connection(self) -> AsyncGenerator[asyncpg.Connection, None]:
        """Get database connection from pool with automatic cleanup"""
        if not self.pool:
            await self.initialize()
        
        async with self.pool.acquire() as connection:
            try:
                yield connection
            except Exception as e:
                # Log database errors for monitoring
                logger.error(f"Database operation failed: {e}")
                raise
    
    async def execute_query(
        self, 
        query: str, 
        *args, 
        fetch_one: bool = False, 
        fetch_all: bool = False
    ):
        """
        Execute query with performance monitoring
        
        🚀 Performance features:
        - Query execution time tracking
        - Automatic query plan caching
        - Connection health monitoring
        """
        start_time = time.time()
        query_hash = hashlib.md5(query.encode()).hexdigest()[:8]
        
        try:
            async with self.get_connection() as conn:
                if fetch_one:
                    result = await conn.fetchrow(query, *args)
                elif fetch_all:
                    result = await conn.fetch(query, *args)
                else:
                    result = await conn.execute(query, *args)
                
                # Track query performance
                execution_time = time.time() - start_time
                self._track_query_performance(query_hash, execution_time)
                
                return result
                
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"Query failed ({query_hash}) after {execution_time:.2f}s: {e}")
            raise
    
    def _track_query_performance(self, query_hash: str, execution_time: float):
        """Track query performance statistics"""
        if query_hash not in self.query_stats:
            self.query_stats[query_hash] = {
                "count": 0,
                "total_time": 0,
                "avg_time": 0,
                "max_time": 0,
                "min_time": float('inf')
            }
        
        stats = self.query_stats[query_hash]
        stats["count"] += 1
        stats["total_time"] += execution_time
        stats["avg_time"] = stats["total_time"] / stats["count"]
        stats["max_time"] = max(stats["max_time"], execution_time)
        stats["min_time"] = min(stats["min_time"], execution_time)
    
    async def close(self):
        """Close database connection pool"""
        if self.pool:
            await self.pool.close()
            logger.info("Database pool closed")

# Global database manager
db_manager = DatabaseManager("postgresql://user:pass@localhost/db")

@app.on_event("startup")
async def startup_event():
    """Initialize database on application startup"""
    await db_manager.initialize()

@app.on_event("shutdown")
async def shutdown_event():
    """Close database connections on shutdown"""
    await db_manager.close()

# High-performance endpoint with optimized database access
@app.get("/users/optimized-db")
async def get_users_optimized_db(
    page: int = Query(1, ge=1),
    limit: int = Query(10, ge=1, le=100),
    search: Optional[str] = Query(None, description="Search by username or email")
):
    """
    🚀 High-performance user listing with database optimization
    
    Performance optimizations:
    - Connection pooling for concurrent requests
    - Optimized SQL with proper indexing
    - Prepared statements to prevent SQL injection
    - Result set limiting to prevent memory issues
    """
    
    offset = (page - 1) * limit
    
    # Build optimized query with proper indexing
    base_query = """
        SELECT u.id, u.username, u.email, u.created_at,
               COUNT(*) OVER() as total_count
        FROM users u
    """
    
    params = []
    where_conditions = []
    
    # Add search functionality with full-text search
    if search:
        where_conditions.append("(u.username ILIKE $1 OR u.email ILIKE $1)")
        params.append(f"%{search}%")
    
    # Add active user filter
    where_conditions.append("u.is_active = TRUE")
    
    if where_conditions:
        base_query += " WHERE " + " AND ".join(where_conditions)
    
    # Add ordering and pagination
    base_query += f" ORDER BY u.created_at DESC LIMIT ${len(params) + 1} OFFSET ${len(params) + 2}"
    params.extend([limit, offset])
    
    # Execute optimized query
    start_time = time.time()
    rows = await db_manager.execute_query(base_query, *params, fetch_all=True)
    query_time = time.time() - start_time
    
    # Process results
    users = []
    total_count = 0
    
    for row in rows:
        users.append({
            "id": row["id"],
            "username": row["username"],
            "email": row["email"],
            "created_at": row["created_at"].isoformat()
        })
        total_count = row["total_count"]  # Same for all rows
    
    # Calculate pagination metadata
    total_pages = (total_count + limit - 1) // limit
    has_next = page < total_pages
    has_prev = page > 1
    
    return {
        "data": users,
        "pagination": {
            "page": page,
            "limit": limit,
            "total_count": total_count,
            "total_pages": total_pages,
            "has_next": has_next,
            "has_prev": has_prev
        },
        "performance": {
            "query_time": f"{query_time * 1000:.2f}ms",
            "active_connections": db_manager.pool.get_size() if db_manager.pool else 0,
            "optimization_notes": [
                "Using connection pooling for concurrent access",
                "Prepared statements prevent SQL injection",
                "ILIKE with indexes for efficient text search",
                "COUNT(*) OVER() avoids separate count query"
            ]
        }
    }
```

## ⚡ Response Streaming & Compression

### 🌊 Streaming Large Datasets

```python
from fastapi.responses import StreamingResponse
import csv
from io import StringIO
import gzip

class DataStreamer:
    """
    🌊 High-performance data streaming for large datasets
    
    Features:
    - Memory-efficient data streaming
    - On-the-fly compression
    - Progress tracking
    - Graceful error handling
    """
    
    def __init__(self, chunk_size: int = 8192):
        self.chunk_size = chunk_size
    
    async def stream_csv_data(
        self, 
        query: str, 
        params: list = None,
        compress: bool = False
    ):
        """
        Stream CSV data from database query
        
        🚀 Memory efficiency: Processes data in chunks rather than loading everything into memory
        """
        if params is None:
            params = []
        
        # CSV buffer for streaming
        csv_buffer = StringIO()
        
        try:
            async with db_manager.get_connection() as conn:
                # Use server-side cursor for large datasets
                async with conn.transaction():
                    cursor = await conn.cursor(query, *params)
                    
                    # Write CSV header
                    first_row = await cursor.fetchrow()
                    if first_row:
                        writer = csv.DictWriter(csv_buffer, fieldnames=first_row.keys())
                        writer.writeheader()
                        
                        # Write first row
                        writer.writerow(dict(first_row))
                        yield csv_buffer.getvalue()
                        csv_buffer.seek(0)
                        csv_buffer.truncate(0)
                        
                        # Stream remaining rows in chunks
                        row_count = 1
                        chunk_rows = []
                        
                        async for row in cursor:
                            chunk_rows.append(dict(row))
                            row_count += 1
                            
                            # Yield chunk when buffer is full
                            if len(chunk_rows) >= 100:  # 100 rows per chunk
                                for chunk_row in chunk_rows:
                                    writer.writerow(chunk_row)
                                
                                chunk_data = csv_buffer.getvalue()
                                csv_buffer.seek(0)
                                csv_buffer.truncate(0)
                                
                                if compress:
                                    chunk_data = gzip.compress(chunk_data.encode()).decode('latin1')
                                
                                yield chunk_data
                                chunk_rows = []
                        
                        # Yield remaining rows
                        if chunk_rows:
                            for chunk_row in chunk_rows:
                                writer.writerow(chunk_row)
                            
                            chunk_data = csv_buffer.getvalue()
                            if compress:
                                chunk_data = gzip.compress(chunk_data.encode()).decode('latin1')
                            
                            yield chunk_data
                        
                        logger.info(f"Streamed {row_count} rows successfully")
                
        except Exception as e:
            logger.error(f"Error streaming data: {e}")
            yield f"Error: {str(e)}\n"
        finally:
            csv_buffer.close()

# Global data streamer
data_streamer = DataStreamer()

@app.get("/export/users")
async def export_users(
    format: str = Query("csv", description="Export format: csv, json"),
    compress: bool = Query(False, description="Enable gzip compression"),
    search: Optional[str] = Query(None, description="Filter by username or email")
):
    """
    🌊 Stream large dataset export with memory efficiency
    
    Performance benefits:
    - Memory usage remains constant regardless of dataset size
    - Client receives data immediately (no waiting for full processing)
    - Server can handle multiple concurrent large exports
    - Automatic compression reduces bandwidth usage
    """
    
    # Build query for export
    query = """
        SELECT id, username, email, created_at, last_login, status
        FROM users
        WHERE is_active = TRUE
    """
    params = []
    
    if search:
        query += " AND (username ILIKE $1 OR email ILIKE $1)"
        params.append(f"%{search}%")
    
    query += " ORDER BY created_at DESC"
    
    if format.lower() == "csv":
        # Set appropriate headers for CSV download
        headers = {
            "Content-Disposition": "attachment; filename=users_export.csv",
            "Content-Type": "text/csv"
        }
        
        if compress:
            headers["Content-Encoding"] = "gzip"
            headers["Content-Type"] = "application/gzip"
        
        return StreamingResponse(
            data_streamer.stream_csv_data(query, params, compress=compress),
            headers=headers,
            media_type="text/csv"
        )
    
    else:
        raise HTTPException(status_code=400, detail="Unsupported export format")
```

## 📊 Performance Monitoring & Metrics

### 🔍 Real-time Performance Analytics

```python
import psutil
from datetime import datetime, timedelta
from collections import deque
import statistics

class PerformanceMonitor:
    """
    📊 Comprehensive performance monitoring for production systems
    
    Tracks:
    - Request response times
    - System resource usage
    - Database performance
    - Cache hit rates
    - Error rates and patterns
    """
    
    def __init__(self, max_samples: int = 1000):
        self.max_samples = max_samples
        self.response_times = deque(maxlen=max_samples)
        self.error_counts = {"4xx": 0, "5xx": 0}
        self.endpoint_stats = {}
        self.start_time = datetime.utcnow()
        
    def record_request(self, endpoint: str, method: str, status_code: int, response_time: float):
        """Record request metrics"""
        self.response_times.append(response_time)
        
        # Track error rates
        if 400 <= status_code < 500:
            self.error_counts["4xx"] += 1
        elif status_code >= 500:
            self.error_counts["5xx"] += 1
        
        # Track per-endpoint statistics
        endpoint_key = f"{method} {endpoint}"
        if endpoint_key not in self.endpoint_stats:
            self.endpoint_stats[endpoint_key] = {
                "count": 0,
                "total_time": 0,
                "avg_time": 0,
                "min_time": float('inf'),
                "max_time": 0,
                "errors": 0
            }
        
        stats = self.endpoint_stats[endpoint_key]
        stats["count"] += 1
        stats["total_time"] += response_time
        stats["avg_time"] = stats["total_time"] / stats["count"]
        stats["min_time"] = min(stats["min_time"], response_time)
        stats["max_time"] = max(stats["max_time"], response_time)
        
        if status_code >= 400:
            stats["errors"] += 1
    
    def get_system_metrics(self) -> Dict[str, Any]:
        """Get current system resource usage"""
        return {
            "cpu_usage": psutil.cpu_percent(interval=1),
            "memory_usage": psutil.virtual_memory().percent,
            "disk_usage": psutil.disk_usage('/').percent,
            "active_connections": len(psutil.net_connections()),
            "load_average": psutil.getloadavg() if hasattr(psutil, 'getloadavg') else None
        }
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """Get comprehensive performance summary"""
        total_requests = len(self.response_times)
        uptime = datetime.utcnow() - self.start_time
        
        if total_requests > 0:
            response_time_stats = {
                "avg": statistics.mean(self.response_times),
                "median": statistics.median(self.response_times),
                "p95": statistics.quantiles(self.response_times, n=20)[18] if total_requests > 20 else 0,
                "p99": statistics.quantiles(self.response_times, n=100)[98] if total_requests > 100 else 0,
                "min": min(self.response_times),
                "max": max(self.response_times)
            }
        else:
            response_time_stats = {}
        
        # Calculate requests per second
        rps = total_requests / uptime.total_seconds() if uptime.total_seconds() > 0 else 0
        
        return {
            "uptime": str(uptime),
            "total_requests": total_requests,
            "requests_per_second": round(rps, 2),
            "response_times": response_time_stats,
            "error_rates": {
                "4xx_rate": f"{(self.error_counts['4xx'] / total_requests * 100):.2f}%" if total_requests > 0 else "0%",
                "5xx_rate": f"{(self.error_counts['5xx'] / total_requests * 100):.2f}%" if total_requests > 0 else "0%"
            },
            "cache_stats": redis_client.get_stats(),
            "system_metrics": self.get_system_metrics(),
            "top_endpoints": sorted(
                self.endpoint_stats.items(),
                key=lambda x: x[1]["count"],
                reverse=True
            )[:10]
        }

# Global performance monitor
perf_monitor = PerformanceMonitor()

# Performance monitoring middleware
@app.middleware("http")
async def performance_monitoring_middleware(request: Request, call_next):
    """Track performance metrics for all requests"""
    start_time = time.time()
    
    try:
        response = await call_next(request)
        
        # Record successful request
        response_time = time.time() - start_time
        perf_monitor.record_request(
            endpoint=request.url.path,
            method=request.method,
            status_code=response.status_code,
            response_time=response_time
        )
        
        # Add performance headers
        response.headers["X-Response-Time"] = f"{response_time * 1000:.2f}ms"
        
        return response
        
    except Exception as e:
        # Record failed request
        response_time = time.time() - start_time
        perf_monitor.record_request(
            endpoint=request.url.path,
            method=request.method,
            status_code=500,
            response_time=response_time
        )
        raise

@app.get("/metrics/performance")
async def get_performance_metrics():
    """
    📊 Real-time performance metrics endpoint
    
    Provides comprehensive insights into:
    - Response time percentiles
    - Error rates and patterns
    - System resource usage
    - Cache performance
    - Database query statistics
    """
    return perf_monitor.get_performance_summary()

@app.get("/health")
async def health_check():
    """
    🏥 Comprehensive health check for load balancers
    
    Returns detailed health status including:
    - Database connectivity
    - Cache availability
    - System resource thresholds
    - Error rate thresholds
    """
    health_status = {"status": "healthy", "checks": {}}
    
    # Check database connectivity
    try:
        await db_manager.execute_query("SELECT 1", fetch_one=True)
        health_status["checks"]["database"] = "healthy"
    except Exception as e:
        health_status["checks"]["database"] = f"unhealthy: {str(e)}"
        health_status["status"] = "unhealthy"
    
    # Check cache availability
    try:
        redis_client.set("health_check", "ok", ex=60)
        cache_test = redis_client.get("health_check")
        health_status["checks"]["cache"] = "healthy" if cache_test else "degraded"
    except Exception as e:
        health_status["checks"]["cache"] = f"unhealthy: {str(e)}"
    
    # Check system resources
    system_metrics = perf_monitor.get_system_metrics()
    if system_metrics["cpu_usage"] > 90 or system_metrics["memory_usage"] > 90:
        health_status["status"] = "degraded"
        health_status["checks"]["resources"] = "high_usage"
    else:
        health_status["checks"]["resources"] = "healthy"
    
    return health_status
```

## 🎯 Enterprise Performance Best Practices

### 🏆 Optimization Strategies Summary

| Strategy | Performance Gain | Implementation Complexity | Use Case |
|----------|------------------|---------------------------|----------|
| **Multi-layer Caching** | 🚀🚀🚀 | 🔧🔧 | High-read workloads |
| **Database Connection Pooling** | 🚀🚀🚀 | 🔧🔧 | Concurrent database access |
| **Response Streaming** | 🚀🚀 | 🔧🔧🔧 | Large dataset exports |
| **Query Optimization** | 🚀🚀🚀 | 🔧🔧 | Database-heavy applications |
| **Async Processing** | 🚀🚀 | 🔧🔧 | I/O bound operations |

### 🚀 Production Performance Checklist

✅ **Caching Strategy**
- Multi-layer caching (memory + Redis)
- Smart cache invalidation
- Cache performance monitoring

✅ **Database Optimization**
- Connection pooling with proper sizing
- Query optimization and indexing
- Read replicas for read-heavy workloads

✅ **Response Optimization**
- Gzip compression for large responses
- Streaming for large datasets
- CDN for static content

✅ **Monitoring & Alerting**
- Real-time performance metrics
- Proactive alerting on degradation
- Comprehensive health checks

### 🔮 Advanced Performance Patterns

After mastering these patterns, consider these next-level optimizations:

1. **🔄 Microservices Patterns**
   - Circuit breakers for service resilience
   - Request batching and multiplexing
   - Distributed caching strategies

2. **⚡ Edge Computing**
   - Edge caching with CDNs
   - Serverless function optimization
   - Geographic request routing

3. **📊 Observability**
   - Distributed tracing (OpenTelemetry)
   - Custom metrics and dashboards
   - Performance regression detection

---

> ⚡ **Performance is a feature**: Users perceive fast applications as more reliable and trustworthy. These patterns ensure your FastAPI application can scale from thousands to millions of users.

Ready to explore enterprise architecture patterns? Continue to [Enterprise Patterns](/docs/05_enterprise)! 🏢