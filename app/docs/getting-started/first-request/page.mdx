# Your First Request

Let's make your first API request to Claude and explore the response structure.

## Basic Request

Here's the simplest way to interact with Claude:

```python
from anthropic import Anthropic

client = Anthropic()

message = client.messages.create(
    model="claude-3-opus-20240229",
    max_tokens=1024,
    messages=[
        {"role": "user", "content": "Hello, Claude! Tell me a joke."}
    ]
)

print(message.content[0].text)
```

## Understanding the Request

Let's break down each parameter:

### model
The Claude model to use. Options include:
- `claude-3-opus-20240229` - Most capable
- `claude-3-sonnet-20240229` - Balanced performance
- `claude-3-haiku-20240307` - Fastest response

### max_tokens
Maximum number of tokens in the response. One token ≈ 4 characters.

### messages
A list of message dictionaries with:
- `role`: Either "user" or "assistant"
- `content`: The message text

## Full Example with Error Handling

```python
from anthropic import Anthropic, APIError
import json

def chat_with_claude(prompt):
    client = Anthropic()
    
    try:
        message = client.messages.create(
            model="claude-3-opus-20240229",
            max_tokens=1024,
            temperature=0.7,
            messages=[
                {"role": "user", "content": prompt}
            ]
        )
        
        return message.content[0].text
        
    except APIError as e:
        print(f"API Error: {e.status_code}")
        print(f"Message: {e.message}")
        return None
    except Exception as e:
        print(f"Unexpected error: {e}")
        return None

# Use the function
response = chat_with_claude("What's the capital of France?")
if response:
    print(response)
```

## Understanding the Response

The response object contains:

```python
# Access the response
print(message.content[0].text)  # The actual response text
print(message.model)  # Model used
print(message.usage.input_tokens)  # Tokens in request
print(message.usage.output_tokens)  # Tokens in response
print(message.stop_reason)  # Why generation stopped
```

### Response Structure

```python
{
    "id": "msg_01XFDUDYJgAACzvnptvVoYEL",
    "type": "message",
    "role": "assistant",
    "content": [
        {
            "type": "text",
            "text": "The capital of France is Paris."
        }
    ],
    "model": "claude-3-opus-20240229",
    "stop_reason": "end_turn",
    "stop_sequence": null,
    "usage": {
        "input_tokens": 10,
        "output_tokens": 8
    }
}
```

## Common First Requests

### 1. Simple Question

```python
response = client.messages.create(
    model="claude-3-opus-20240229",
    max_tokens=100,
    messages=[
        {"role": "user", "content": "What is 2+2?"}
    ]
)
print(response.content[0].text)  # "2 + 2 = 4"
```

### 2. Code Generation

```python
response = client.messages.create(
    model="claude-3-opus-20240229",
    max_tokens=500,
    messages=[
        {"role": "user", "content": "Write a Python function to calculate factorial"}
    ]
)
print(response.content[0].text)
```

### 3. Text Analysis

```python
text = "The quick brown fox jumps over the lazy dog."
response = client.messages.create(
    model="claude-3-opus-20240229",
    max_tokens=200,
    messages=[
        {"role": "user", "content": f"Count the words in this text: {text}"}
    ]
)
print(response.content[0].text)
```

## Adding System Messages

System messages help define Claude's behavior:

```python
response = client.messages.create(
    model="claude-3-opus-20240229",
    max_tokens=200,
    system="You are a helpful assistant who speaks like a pirate.",
    messages=[
        {"role": "user", "content": "Tell me about Python programming"}
    ]
)
print(response.content[0].text)
```

## Handling Long Responses

For longer responses, increase max_tokens:

```python
response = client.messages.create(
    model="claude-3-opus-20240229",
    max_tokens=4096,  # Maximum for most models
    messages=[
        {"role": "user", "content": "Explain quantum computing in detail"}
    ]
)
```

## Next Steps

Now that you've made your first request:

1. Try different models to see performance differences
2. Experiment with temperature settings
3. Build a conversation with multiple messages
4. Explore streaming responses

[← Previous: Authentication](/docs/getting-started/authentication) | [Next: Basic Usage →](/docs/basic-usage)