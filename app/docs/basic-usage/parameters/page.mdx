# Parameters Guide

Understand all the parameters available when making requests to the Anthropic API and how to use them effectively.

## Core Parameters

### model (required)

The Claude model to use for the request:

```python
# Most capable model
model="claude-3-opus-20240229"

# Balanced performance and speed
model="claude-3-sonnet-20240229"

# Fastest, most cost-effective
model="claude-3-haiku-20240307"
```

**Choosing the right model:**

- Use **Opus** for complex reasoning, analysis, and creative tasks
- Use **Sonnet** for general-purpose applications
- Use **Haiku** for simple tasks or when speed matters

### max_tokens (required)

Maximum tokens in the response:

```python
# Short response
max_tokens=100

# Medium response
max_tokens=1024

# Long response
max_tokens=4096  # Maximum for most models
```

**Token estimation:**
- 1 token ≈ 4 characters in English
- 1 token ≈ 0.75 words on average
- Consider both input + output tokens for billing

### messages (required)

The conversation history:

```python
messages=[
    {"role": "user", "content": "Hello!"},
    {"role": "assistant", "content": "Hi there!"},
    {"role": "user", "content": "How are you?"}
]
```

**Message roles:**
- `user`: Human messages
- `assistant`: Claude's responses
- Must alternate between user and assistant
- Must end with a user message

## Optional Parameters

### system

Define Claude's behavior and personality:

```python
system="You are a helpful assistant who explains things clearly and concisely."

# More specific instructions
system="""You are a Python programming tutor. 
- Always provide working code examples
- Explain concepts step by step
- Ask clarifying questions when needed"""
```

### temperature

Controls randomness in responses (0.0 to 1.0):

```python
# Deterministic, focused responses
temperature=0.0

# Balanced creativity and consistency
temperature=0.7

# More creative, varied responses
temperature=1.0
```

**Use cases:**
- `0.0-0.3`: Code generation, factual Q&A, analysis
- `0.4-0.7`: General conversation, explanations
- `0.8-1.0`: Creative writing, brainstorming

### top_p

Alternative to temperature using nucleus sampling:

```python
# Only consider tokens in top 10% probability
top_p=0.1

# Consider tokens in top 90% probability
top_p=0.9
```

**Note:** Use either `temperature` OR `top_p`, not both.

### top_k

Limit vocabulary to top K most likely tokens:

```python
# Conservative vocabulary
top_k=10

# Larger vocabulary
top_k=50
```

### stop_sequences

Stop generation at specific strings:

```python
stop_sequences=["END", "---", "\n\n"]

# Example with code blocks
stop_sequences=["```"]
```

## Parameter Examples

### Creative Writing

```python
response = client.messages.create(
    model="claude-3-opus-20240229",
    max_tokens=2048,
    temperature=0.9,  # High creativity
    system="You are a creative fiction writer with a vivid imagination.",
    messages=[
        {"role": "user", "content": "Write a short story about a time traveler"}
    ]
)
```

### Code Generation

```python
response = client.messages.create(
    model="claude-3-opus-20240229",
    max_tokens=1024,
    temperature=0.1,  # Low for consistency
    system="You are an expert programmer. Provide clean, well-commented code.",
    messages=[
        {"role": "user", "content": "Create a Python function to sort a list of dictionaries by a key"}
    ]
)
```

### Technical Analysis

```python
response = client.messages.create(
    model="claude-3-opus-20240229",
    max_tokens=2048,
    temperature=0.3,  # Focused but allowing some flexibility
    system="You are a technical analyst. Provide detailed, accurate analysis with supporting evidence.",
    messages=[
        {"role": "user", "content": "Analyze the pros and cons of microservices architecture"}
    ]
)
```

### Educational Content

```python
response = client.messages.create(
    model="claude-3-sonnet-20240229",
    max_tokens=1500,
    temperature=0.5,
    system="You are a patient teacher. Break down complex topics into simple, understandable parts.",
    messages=[
        {"role": "user", "content": "Explain quantum computing to a beginner"}
    ]
)
```

## Advanced Parameter Usage

### Dynamic Parameter Selection

Choose parameters based on task type:

```python
def get_parameters(task_type):
    params = {
        "creative": {
            "model": "claude-3-opus-20240229",
            "temperature": 0.8,
            "max_tokens": 2048
        },
        "analytical": {
            "model": "claude-3-opus-20240229", 
            "temperature": 0.2,
            "max_tokens": 2048
        },
        "coding": {
            "model": "claude-3-opus-20240229",
            "temperature": 0.1,
            "max_tokens": 1024
        },
        "quick": {
            "model": "claude-3-haiku-20240307",
            "temperature": 0.5,
            "max_tokens": 512
        }
    }
    return params.get(task_type, params["analytical"])

# Usage
params = get_parameters("coding")
response = client.messages.create(
    messages=[{"role": "user", "content": "Write a sorting algorithm"}],
    **params
)
```

### Conversation-Aware Parameters

Adjust parameters based on conversation context:

```python
class SmartConversation:
    def __init__(self):
        self.client = Anthropic()
        self.messages = []
        self.turn_count = 0
    
    def send_message(self, content):
        self.turn_count += 1
        self.messages.append({"role": "user", "content": content})
        
        # Adjust temperature based on conversation length
        if self.turn_count == 1:
            temp = 0.3  # Start focused
        elif self.turn_count < 5:
            temp = 0.5  # Become more conversational
        else:
            temp = 0.7  # Allow more creativity in longer conversations
        
        # Adjust max_tokens based on content type
        if any(keyword in content.lower() for keyword in ["explain", "describe", "analyze"]):
            max_tokens = 2048
        elif any(keyword in content.lower() for keyword in ["code", "function", "class"]):
            max_tokens = 1024
        else:
            max_tokens = 512
        
        response = self.client.messages.create(
            model="claude-3-opus-20240229",
            max_tokens=max_tokens,
            temperature=temp,
            messages=self.messages
        )
        
        assistant_response = response.content[0].text
        self.messages.append({"role": "assistant", "content": assistant_response})
        
        return assistant_response
```

## Parameter Validation

Validate parameters before making requests:

```python
def validate_parameters(**kwargs):
    """Validate API parameters"""
    errors = []
    
    # Check required parameters
    required = ['model', 'max_tokens', 'messages']
    for param in required:
        if param not in kwargs:
            errors.append(f"Missing required parameter: {param}")
    
    # Validate ranges
    if 'temperature' in kwargs:
        temp = kwargs['temperature']
        if not 0.0 <= temp <= 1.0:
            errors.append("Temperature must be between 0.0 and 1.0")
    
    if 'top_p' in kwargs:
        top_p = kwargs['top_p']
        if not 0.0 <= top_p <= 1.0:
            errors.append("top_p must be between 0.0 and 1.0")
    
    if 'max_tokens' in kwargs:
        max_tokens = kwargs['max_tokens']
        if max_tokens <= 0:
            errors.append("max_tokens must be positive")
        if max_tokens > 4096:
            errors.append("max_tokens cannot exceed 4096")
    
    # Check for conflicting parameters
    if 'temperature' in kwargs and 'top_p' in kwargs:
        errors.append("Cannot use both temperature and top_p")
    
    if errors:
        raise ValueError(f"Parameter validation failed: {'; '.join(errors)}")
    
    return True

# Usage
try:
    validate_parameters(
        model="claude-3-opus-20240229",
        max_tokens=1024,
        temperature=0.5,
        messages=[{"role": "user", "content": "Hello"}]
    )
    print("Parameters valid!")
except ValueError as e:
    print(f"Error: {e}")
```

## Best Practices

1. **Start with defaults**: Begin with standard parameters and adjust based on results
2. **Test systematically**: Change one parameter at a time to understand effects
3. **Consider costs**: Higher max_tokens and more capable models cost more
4. **Match task to parameters**: Use appropriate settings for your specific use case
5. **Monitor performance**: Track how parameter changes affect response quality

[← Previous: Conversations](/docs/basic-usage/conversations) | [Next: Responses →](/docs/basic-usage/responses)