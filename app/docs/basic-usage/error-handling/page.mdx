# Error Handling

Learn how to handle errors gracefully when working with the Anthropic API, including common error types and recovery strategies.

## Common Error Types

The Anthropic SDK raises different exceptions for various error conditions:

### APIError

Base class for all API-related errors:

```python
from anthropic import APIError

try:
    response = client.messages.create(
        model="claude-3-opus-20240229",
        max_tokens=1024,
        messages=[{"role": "user", "content": "Hello"}]
    )
except APIError as e:
    print(f"API Error: {e.status_code}")
    print(f"Message: {e.message}")
    print(f"Type: {e.type}")
```

### Specific Error Types

```python
from anthropic import (
    APIError,
    AuthenticationError,
    BadRequestError, 
    InternalServerError,
    NotFoundError,
    PermissionDeniedError,
    RateLimitError,
    UnprocessableEntityError
)

try:
    response = client.messages.create(...)
    
except AuthenticationError:
    print("Invalid API key")
    
except RateLimitError as e:
    print(f"Rate limit exceeded. Retry after: {e.retry_after}")
    
except BadRequestError as e:
    print(f"Bad request: {e.message}")
    
except InternalServerError:
    print("Server error - try again later")
    
except APIError as e:
    print(f"Other API error: {e}")
```

## Comprehensive Error Handling

### Robust API Call Function

```python
import time
import random
from anthropic import APIError, RateLimitError, InternalServerError

def robust_api_call(client, max_retries=3, base_delay=1, **kwargs):
    """
    Make an API call with robust error handling and retries
    """
    for attempt in range(max_retries + 1):
        try:
            response = client.messages.create(**kwargs)
            return response
            
        except RateLimitError as e:
            if attempt == max_retries:
                raise
            
            # Use retry_after if provided, otherwise exponential backoff
            delay = e.retry_after if hasattr(e, 'retry_after') else base_delay * (2 ** attempt)
            print(f"Rate limited. Waiting {delay} seconds...")
            time.sleep(delay)
            
        except InternalServerError as e:
            if attempt == max_retries:
                raise
            
            # Exponential backoff with jitter for server errors
            delay = base_delay * (2 ** attempt) + random.uniform(0, 1)
            print(f"Server error. Retrying in {delay:.2f} seconds...")
            time.sleep(delay)
            
        except (AuthenticationError, BadRequestError, PermissionDeniedError):
            # Don't retry these errors - they won't succeed
            raise
            
        except APIError as e:
            if attempt == max_retries:
                raise
            
            print(f"API error on attempt {attempt + 1}: {e}")
            time.sleep(base_delay * (attempt + 1))

# Usage
try:
    response = robust_api_call(
        client,
        model="claude-3-opus-20240229",
        max_tokens=1024,
        messages=[{"role": "user", "content": "Hello"}]
    )
    print("Success:", response.content[0].text)
    
except APIError as e:
    print(f"Failed after retries: {e}")
```

### Error Context Manager

```python
from contextlib import contextmanager
import logging

@contextmanager
def api_error_handler(operation_name="API call"):
    """Context manager for consistent error handling"""
    try:
        yield
    except AuthenticationError:
        logging.error(f"{operation_name}: Authentication failed - check API key")
        raise
    except RateLimitError as e:
        logging.warning(f"{operation_name}: Rate limited - {e}")
        raise
    except BadRequestError as e:
        logging.error(f"{operation_name}: Bad request - {e.message}")
        raise
    except InternalServerError:
        logging.error(f"{operation_name}: Server error")
        raise
    except APIError as e:
        logging.error(f"{operation_name}: API error - {e}")
        raise
    except Exception as e:
        logging.error(f"{operation_name}: Unexpected error - {e}")
        raise

# Usage
with api_error_handler("Creating message"):
    response = client.messages.create(
        model="claude-3-opus-20240229",
        max_tokens=1024,
        messages=[{"role": "user", "content": "Hello"}]
    )
```

## Rate Limit Handling

### Smart Rate Limiting

```python
import time
from collections import deque

class RateLimiter:
    def __init__(self, requests_per_minute=60):
        self.requests_per_minute = requests_per_minute
        self.requests = deque()
    
    def wait_if_needed(self):
        """Wait if we're hitting rate limits"""
        now = time.time()
        
        # Remove requests older than 1 minute
        while self.requests and now - self.requests[0] > 60:
            self.requests.popleft()
        
        # If we're at the limit, wait
        if len(self.requests) >= self.requests_per_minute:
            sleep_time = 60 - (now - self.requests[0])
            if sleep_time > 0:
                print(f"Rate limit approached. Waiting {sleep_time:.2f} seconds...")
                time.sleep(sleep_time)
        
        # Record this request
        self.requests.append(now)

# Usage
limiter = RateLimiter(requests_per_minute=50)  # Conservative limit

def rate_limited_request(client, **kwargs):
    limiter.wait_if_needed()
    return client.messages.create(**kwargs)
```

### Adaptive Rate Limiting

```python
class AdaptiveRateLimiter:
    def __init__(self, initial_rpm=60):
        self.current_rpm = initial_rpm
        self.success_count = 0
        self.error_count = 0
        self.last_request_time = 0
    
    def handle_success(self):
        """Handle successful request"""
        self.success_count += 1
        if self.success_count > 10 and self.error_count == 0:
            # Gradually increase rate if we're consistently successful
            self.current_rpm = min(self.current_rpm * 1.1, 100)
            self.success_count = 0
    
    def handle_rate_limit(self):
        """Handle rate limit error"""
        self.error_count += 1
        # Reduce rate when we hit limits
        self.current_rpm = max(self.current_rpm * 0.5, 10)
        self.success_count = 0
    
    def wait_if_needed(self):
        """Wait based on current rate limit"""
        if self.last_request_time > 0:
            time_since_last = time.time() - self.last_request_time
            min_interval = 60 / self.current_rpm
            if time_since_last < min_interval:
                sleep_time = min_interval - time_since_last
                time.sleep(sleep_time)
        
        self.last_request_time = time.time()

def adaptive_request(client, **kwargs):
    while True:
        try:
            limiter.wait_if_needed()
            response = client.messages.create(**kwargs)
            limiter.handle_success()
            return response
        except RateLimitError:
            limiter.handle_rate_limit()
            time.sleep(60)  # Wait a minute before retrying
```

## Error Recovery Strategies

### Graceful Degradation

```python
def get_response_with_fallback(client, prompt, primary_model="claude-3-opus-20240229"):
    """Try primary model, fall back to alternatives on error"""
    
    models_to_try = [
        primary_model,
        "claude-3-sonnet-20240229",
        "claude-3-haiku-20240307"
    ]
    
    for model in models_to_try:
        try:
            response = client.messages.create(
                model=model,
                max_tokens=1024,
                messages=[{"role": "user", "content": prompt}]
            )
            return response, model
            
        except (RateLimitError, InternalServerError):
            print(f"Error with {model}, trying next...")
            continue
            
        except (AuthenticationError, BadRequestError):
            # These errors won't be fixed by trying different models
            raise
    
    raise Exception("All models failed")

# Usage
try:
    response, used_model = get_response_with_fallback(client, "Hello!")
    print(f"Success with {used_model}: {response.content[0].text}")
except Exception as e:
    print(f"All requests failed: {e}")
```

### Request Validation

```python
def validate_request_params(**kwargs):
    """Validate request parameters before making API call"""
    errors = []
    
    # Check required parameters
    if 'messages' not in kwargs:
        errors.append("messages parameter is required")
    elif not kwargs['messages']:
        errors.append("messages cannot be empty")
    
    if 'model' not in kwargs:
        errors.append("model parameter is required")
    
    if 'max_tokens' not in kwargs:
        errors.append("max_tokens parameter is required")
    
    # Validate message format
    if 'messages' in kwargs:
        for i, msg in enumerate(kwargs['messages']):
            if not isinstance(msg, dict):
                errors.append(f"Message {i} must be a dictionary")
                continue
            
            if 'role' not in msg:
                errors.append(f"Message {i} missing 'role'")
            elif msg['role'] not in ['user', 'assistant']:
                errors.append(f"Message {i} has invalid role: {msg['role']}")
            
            if 'content' not in msg:
                errors.append(f"Message {i} missing 'content'")
            elif not msg['content'].strip():
                errors.append(f"Message {i} has empty content")
    
    # Validate ranges
    if 'temperature' in kwargs:
        temp = kwargs['temperature']
        if not 0 <= temp <= 1:
            errors.append("temperature must be between 0 and 1")
    
    if 'max_tokens' in kwargs:
        max_tokens = kwargs['max_tokens']
        if max_tokens <= 0:
            errors.append("max_tokens must be positive")
    
    return errors

def safe_api_call(client, **kwargs):
    """Validate parameters before making API call"""
    errors = validate_request_params(**kwargs)
    if errors:
        raise ValueError(f"Invalid parameters: {'; '.join(errors)}")
    
    return client.messages.create(**kwargs)
```

## Logging and Monitoring

### Request Logging

```python
import logging
import json
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def logged_api_call(client, **kwargs):
    """API call with comprehensive logging"""
    start_time = datetime.now()
    request_id = f"req_{int(start_time.timestamp())}"
    
    # Log request
    logging.info(f"[{request_id}] Starting API request")
    logging.debug(f"[{request_id}] Parameters: {json.dumps(kwargs, default=str)}")
    
    try:
        response = client.messages.create(**kwargs)
        
        # Log success
        duration = (datetime.now() - start_time).total_seconds()
        logging.info(f"[{request_id}] Success - Duration: {duration:.2f}s, "
                    f"Tokens: {response.usage.input_tokens} in, "
                    f"{response.usage.output_tokens} out")
        
        return response
        
    except APIError as e:
        duration = (datetime.now() - start_time).total_seconds()
        logging.error(f"[{request_id}] API Error after {duration:.2f}s: "
                     f"{e.status_code} - {e.message}")
        raise
        
    except Exception as e:
        duration = (datetime.now() - start_time).total_seconds()
        logging.error(f"[{request_id}] Unexpected error after {duration:.2f}s: {e}")
        raise
```

## Best Practices

1. **Always wrap API calls** in try-catch blocks
2. **Implement exponential backoff** for retryable errors
3. **Don't retry** authentication or bad request errors
4. **Log errors** with sufficient context for debugging
5. **Validate inputs** before making API calls
6. **Monitor rate limits** and implement adaptive throttling
7. **Have fallback strategies** for critical applications
8. **Test error scenarios** in your application

[← Previous: Responses](/docs/basic-usage/responses) | [Next: Advanced Features →](/docs/advanced-features)